{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "MVJsAGsJtznB",
        "outputId": "cdfc8e9b-e98b-40c7-f36a-9dca67fd6054"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--2024-02-05 15:24:24--  https://dataset.cs.mcgill.ca/ubuntu-corpus-1.0/ubuntu_dataset.tgz.aa\n",
            "Resolving dataset.cs.mcgill.ca (dataset.cs.mcgill.ca)... 132.206.51.25\n",
            "Connecting to dataset.cs.mcgill.ca (dataset.cs.mcgill.ca)|132.206.51.25|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1000000000 (954M) [application/octet-stream]\n",
            "Saving to: ‘ubuntu_dataset.tgz.aa.1’\n",
            "\n",
            "ubuntu_dataset.tgz.  48%[========>           ] 463.01M  10.7MB/s    eta 31s    "
          ]
        }
      ],
      "source": [
        "!wget https://dataset.cs.mcgill.ca/ubuntu-corpus-1.0/ubuntu_dataset.tgz.aa\n",
        "!wget https://dataset.cs.mcgill.ca/ubuntu-corpus-1.0/ubuntu_dataset.tgz.ab\n",
        "!wget https://dataset.cs.mcgill.ca/ubuntu-corpus-1.0/ubuntu_dataset.tgz.ac\n",
        "!wget https://dataset.cs.mcgill.ca/ubuntu-corpus-1.0/ubuntu_dataset.tgz.ad\n",
        "!wget https://dataset.cs.mcgill.ca/ubuntu-corpus-1.0/ubuntu_dataset.tgz.ae"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eChQbwvOlmQ8"
      },
      "source": [
        "## load dataset(almost a failure :) )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a4nzqcz58Qid"
      },
      "outputs": [],
      "source": [
        "!cat ubuntu_dataset.tgz.a* | tar xz"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_peLdL5Y-TQO"
      },
      "outputs": [],
      "source": [
        "# !tar -xvf ubuntu_dataset.tgz.aa"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "L0fHIbbRprI-"
      },
      "outputs": [],
      "source": [
        "# import pandas as pd\n",
        "# import gc\n",
        "# # Specify the chunk size\n",
        "# chunk_size = 10**8  # Adjust the chunk size based on your available memory\n",
        "\n",
        "# # Initialize an empty DataFrame to store the concatenated data\n",
        "# concatenated_df = pd.DataFrame()\n",
        "\n",
        "# # Iterate over chunks and process them\n",
        "# for chunk in pd.read_csv('/content/ubuntu_csvfiles/trainset.csv', chunksize=chunk_size):\n",
        "#     # Process each chunk (perform any required operations or filtering)\n",
        "#     # For example, you might want to filter rows or perform computations on the chunk\n",
        "#     processed_chunk = chunk  # Placeholder, replace with your actual processing logic\n",
        "#     gc.collect()\n",
        "#     concatenated_df = pd.concat([concatenated_df, processed_chunk], ignore_index=True)\n",
        "#---------------------------------------------\n",
        "# from dask.distributed import Client, LocalCluster\n",
        "# import dask.dataframe as dd\n",
        "# import pandas as pd\n",
        "# cluster = LocalCluster()\n",
        "# client = Client(cluster)\n",
        "# chunk_size = 10**8\n",
        "\n",
        "# dask_df = dd.read_csv('/content/ubuntu_csvfiles/trainset.csv', blocksize=chunk_size)\n",
        "\n",
        "\n",
        "# # Visualize the Dask DataFrame partitions\n",
        "# # dask_df.visualize()\n",
        "\n",
        "# # Perform operations on Dask DataFrame\n",
        "\n",
        "\n",
        "# # Convert Dask DataFrame to Pandas DataFrame if needed\n",
        "# result_pandas =dask_df.compute()\n",
        "# client.close()\n",
        "# cluster.close()\n",
        "#-------------------------------# import dask.dataframe as dd\n",
        "# import pandas as pd\n",
        "# chunk_size = 40000  # Adjust the chunk size based on your available memory\n",
        "\n",
        "# # Read the CSV file using Dask and specify the number of partitions\n",
        "# # dask_df = dd.read_csv('/content/ubuntu_csvfiles/trainset.csv', blocksize=chunk_size)\n",
        "\n",
        "\n",
        "# # Visualize the Dask DataFrame partitions\n",
        "# # dask_df.visualize()\n",
        "\n",
        "# # Perform operations on Dask DataFrame\n",
        "\n",
        "\n",
        "# # Convert Dask DataFrame to Pandas DataFrame if needed\n",
        "# result_pandas =dask_df.compute()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7zcVKdXqu4iI"
      },
      "outputs": [],
      "source": [
        "# !tar -xvf ubuntu_blobs.tgz"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tGtYuSn8lv0k"
      },
      "source": [
        "## import 0.25 of data :)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zG2M8bBHK8hU"
      },
      "outputs": [],
      "source": [
        "import dask.dataframe as dd\n",
        "import pandas as pd\n",
        "\n",
        "# Read the entire dataset from the CSV file\n",
        "full_dataset = pd.read_csv('/content/ubuntu_csvfiles/trainset.csv')\n",
        "\n",
        "# Define the desired subset size (adjust as needed)\n",
        "\n",
        "\n",
        "# Save the subset to a new CSV file\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9P_yl6R3GA37"
      },
      "outputs": [],
      "source": [
        "subset_size = int(0.03 * len(full_dataset))  # 25% of the full dataset\n",
        "\n",
        "# Randomly sample a subset from the full dataset\n",
        "mydataset = full_dataset.sample(n=subset_size, random_state=42)  # Set a random seed for reproducibility"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GdevOScXZpf2"
      },
      "outputs": [],
      "source": [
        "print(len(mydataset))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DZfLsS7rfEN8"
      },
      "source": [
        "## Tokenize and preprocess data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kAl2a5XnoVhK"
      },
      "outputs": [],
      "source": [
        "mydataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KlwdWJpt1tt7"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, AdamW, get_linear_schedule_with_warmup\n",
        "import pandas as pd\n",
        "\n",
        "# Load the model and tokenizer\n",
        "model_name = \"microsoft/DialoGPT-medium\"\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "tokenizer.add_special_tokens({'pad_token': '[PAD]'})  # Add a padding token\n",
        "\n",
        "model = AutoModelForCausalLM.from_pretrained(model_name)\n",
        "print(model.config.max_position_embeddings)\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lydB9FAw3reh"
      },
      "outputs": [],
      "source": [
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "# import torch\n",
        "# import torch_xla\n",
        "# import torch_xla.core.xla_model as xm\n",
        "\n",
        "# device = xm.xla_device()\n",
        "# Load your dataset (assuming 'context' and 'utterance' are column names)\n",
        "# Adjust this part based on your actual dataset structure\n",
        "# Replace 'context' and 'utterance' with the actual column names\n",
        "# mydataset = pd.read_csv('/content/ubuntu_csvfiles/trainset.csv')\n",
        "# mydataset = mydataset[['context', 'utterance']]\n",
        "\n",
        "# Define your custom dataset class\n",
        "class MyDataset(Dataset):\n",
        "    def __init__(self, dataframe, tokenizer, max_length=32):\n",
        "        self.data = dataframe\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_length = max_length\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        context = self.data.iloc[idx][0]\n",
        "        response = self.data.iloc[idx][1]\n",
        "\n",
        "        # Truncate or omit tokens if the total length is greater than max_length\n",
        "        input_text = f\"{context} </s> {response}\"\n",
        "        encoding = self.tokenizer(\n",
        "            input_text,\n",
        "            padding=\"max_length\",\n",
        "            truncation=True,\n",
        "            return_tensors=\"pt\",\n",
        "            max_length=self.max_length,\n",
        "        )\n",
        "\n",
        "        return {\n",
        "            'input_ids': encoding['input_ids'].flatten(),\n",
        "            'attention_mask': encoding['attention_mask'].flatten(),\n",
        "            'labels': encoding['input_ids'].flatten()\n",
        "        }\n",
        "\n",
        "\n",
        "custom_dataset = MyDataset(mydataset, tokenizer)\n",
        "# Create a DataLoader\n",
        "data_loader = DataLoader(custom_dataset, batch_size=32, shuffle=True)\n",
        "\n",
        "# Set up training parameters\n",
        "epochs = 2\n",
        "learning_rate = 2e-5\n",
        "\n",
        "# Create optimizer and scheduler\n",
        "optimizer = AdamW(model.parameters(), lr=learning_rate)\n",
        "total_steps = len(data_loader) * epochs\n",
        "scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=0, num_training_steps=total_steps)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w2PVH6vo3tl1"
      },
      "outputs": [],
      "source": [
        "# Training loop\n",
        "model.resize_token_embeddings(len(tokenizer))\n",
        "model.to(device)\n",
        "\n",
        "# export CUDA_LAUNCH_BLOCKING=1\n",
        "# import os\n",
        "\n",
        "# # Set CUDA_LAUNCH_BLOCKING environment variable\n",
        "# os.environ['CUDA_LAUNCH_BLOCKING'] = '0'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XvKkhOBE3Hjc"
      },
      "outputs": [],
      "source": [
        "# Set CUDA_LAUNCH_BLOCKING environment variable\n",
        "import os\n",
        "\n",
        "# model.resize_token_embeddings(len(tokenizer))\n",
        "for epoch in range(epochs):\n",
        "    i = 1\n",
        "    for batch in data_loader:\n",
        "        input_ids = batch['input_ids'].to(device)\n",
        "        attention_mask = batch['attention_mask'].to(device)\n",
        "        labels = batch['labels'].to(device)\n",
        "\n",
        "        # Ensure the length of input_ids is within the model's limit\n",
        "        # if input_ids.size(1) > model.config.max_position_embeddings:\n",
        "        #     input_ids = input_ids[:, :model.config.max_position_embeddings]\n",
        "        # print(input_ids.shape)\n",
        "\n",
        "        outputs = model(input_ids, labels=labels)\n",
        "\n",
        "        loss = outputs.loss\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        scheduler.step()\n",
        "        if(i %1000 == 0):\n",
        "          print(\"1000 batch is passed\")\n",
        "        i+=1\n",
        "    print(\"batch f{batch}\")\n",
        "\n",
        "# Save the pretrained model\n",
        "model.save_pretrained(\"pretrained_dialogpt_model\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YJsyM0-O6a66"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}