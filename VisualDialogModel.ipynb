{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lnRXg9bhu8cL"
      },
      "source": [
        "#import libraties"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vC9_K9ffPkKa"
      },
      "outputs": [],
      "source": [
        "# import locale\n",
        "# locale.getpreferredencoding = lambda: \"UTF-8\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pxr5EPOpu7dG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "74cc3b5e-e8b2-488f-e24a-392c3d41342a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: datasets in /usr/local/lib/python3.10/dist-packages (2.17.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from datasets) (3.13.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from datasets) (1.25.2)\n",
            "Requirement already satisfied: pyarrow>=12.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (14.0.2)\n",
            "Requirement already satisfied: pyarrow-hotfix in /usr/local/lib/python3.10/dist-packages (from datasets) (0.6)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.3.8)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets) (1.5.3)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (2.31.0)\n",
            "Requirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (4.66.2)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from datasets) (3.4.1)\n",
            "Requirement already satisfied: multiprocess in /usr/local/lib/python3.10/dist-packages (from datasets) (0.70.16)\n",
            "Requirement already satisfied: fsspec[http]<=2023.10.0,>=2023.1.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (2023.6.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets) (3.9.3)\n",
            "Requirement already satisfied: huggingface-hub>=0.19.4 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.20.3)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from datasets) (23.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (6.0.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (23.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (6.0.5)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.9.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (4.0.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.19.4->datasets) (4.9.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (2024.2.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2023.4)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.1->pandas->datasets) (1.16.0)\n"
          ]
        }
      ],
      "source": [
        "# !pip install py7zr\n",
        "!pip install datasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fiREBtQgvLvv"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "import numpy as np\n",
        "import os\n",
        "import zipfile\n",
        "from datasets import load_dataset, concatenate_datasets\n",
        "import pandas as pd\n",
        "from bs4 import BeautifulSoup\n",
        "import re\n",
        "import requests\n",
        "from PIL import Image\n",
        "from io import BytesIO\n",
        "from tqdm import tqdm\n",
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "import string\n",
        "from transformers import PreTrainedTokenizer\n",
        "from transformers import BertTokenizer\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, AdamW, get_linear_schedule_with_warmup\n",
        "import torch\n",
        "import torchvision\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torchvision.models as models\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from torchvision import transforms\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torch.nn.utils.rnn import pad_sequence, pack_padded_sequence\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "from torchvision import transforms\n",
        "from torchvision.models import resnet50\n",
        "from torchvision.transforms import ToTensor\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.metrics import accuracy_score\n",
        "from scipy.sparse import vstack\n",
        "from transformers import Pix2StructVisionModel, AutoModelForCausalLM, AutoTokenizer\n",
        "from transformers import AutoProcessor, Pix2StructVisionModel\n",
        "from torch.cuda.amp import autocast, GradScaler\n",
        "import torch.nn.functional as F\n",
        "from torch.cuda.amp import GradScaler, autocast\n",
        "import numpy as np\n",
        "# from sklearn.metrics import ndcg_score, reciprocal_rank_score"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7cKIFIEyeyZN"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import ndcg_score"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8rxUXbpdvQTS"
      },
      "outputs": [],
      "source": [
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5zYQu6HTMfrs",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d5db1ad7-9d4c-46a7-db98-1684dd8f9965"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1\n"
          ]
        }
      ],
      "source": [
        "print(torch.cuda.device_count())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3nSIzRPHvQ4m",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4dc9753d-b7a7-4f70-b288-4282ece02cda"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "neA9jKxvvpgR"
      },
      "source": [
        "# Import dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZC4cd4ODwQ2r"
      },
      "outputs": [],
      "source": [
        "def load_data(directory_path):\n",
        "  return load_dataset('arrow', data_files=directory_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "75xUcN-uvhdM"
      },
      "outputs": [],
      "source": [
        "#Question_dialog_vqa = load_data('/content/drive/MyDrive/Datas/Questions_Dialog/data-00000-of-00001.arrow') #contains dialog answer(only 394)\n",
        "# Answer_dialog_vqa = load_data('/content/drive/MyDrive/Datas/Answers Dialog/data-00000-of-00001.arrow') #just answerdialogs with question and answer\n",
        "# Concat_dialog_vqa = load_data('/content/drive/MyDrive/Datas/Concat_data/data-00000-of-00001.arrow') #contain both dialoganswer and questiondialog(one of them is null)\n",
        "Question_dialog_vqa = load_data('/content/drive/MyDrive/Datas/Questions_Dialog/data-00000-of-00001.arrow') #contains dialog answer(only 394)\n",
        "# # Answer_dialog_vqa = load_data('/content/drive/MyDrive/Datas/Answers Dialog/data-00000-of-00001.arrow') #just answerdialogs with question and answer\n",
        "# # Concat_dialog_vqa = load_data('/content/drive/MyDrive/Datas/Concat_data/data-00000-of-00001.arrow') #contain both dialoganswer and questiondialog(one of them is null)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KEMZIaDh-JPf"
      },
      "outputs": [],
      "source": [
        "columns_to_remove = ['ParentId', 'Score', 'ViewCount', 'ContentLicense',\n",
        "                      'FavoriteCount', 'LastActivityDate', 'LastEditDate', 'LastEditorUserId', 'Tags','PostTypeId','AcceptedAnswerId','Title', 'CreationDate', 'OwnerUserId']\n",
        "Question_dialog_vqa=Question_dialog_vqa['train'].remove_columns(columns_to_remove)\n",
        "# Answer_dialog_vqa=Answer_dialog_vqa['train'].remove_columns(columns_to_remove)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9Kun0MHFvHG1"
      },
      "outputs": [],
      "source": [
        "mydataset =Question_dialog_vqa.map(lambda example: {\"Image\": example[\"Image\"][0]})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pNPBi7K-4zHS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b059b62e-fce3-410b-d0a2-1b1da2d026f0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['https://i.stack.imgur.com/h8Fod.png', 'https://i.stack.imgur.com/TvSPM.png', 'https://i.stack.imgur.com/irw1S.jpg', 'https://i.stack.imgur.com/lBFti.png', 'https://i.stack.imgur.com/GLgbU.jpg']\n"
          ]
        }
      ],
      "source": [
        "print(mydataset['Image'][0:5])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x5b9rh0g2iI4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "aa1a5f04-d82f-4834-cd6e-13295434c441"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset({\n",
            "    features: ['Id', 'Body', 'DialogHistory', 'Image', 'Answer', 'DialogAnswers'],\n",
            "    num_rows: 8024\n",
            "})\n"
          ]
        }
      ],
      "source": [
        "print(mydataset)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aF1EpxcCAZWX"
      },
      "source": [
        "# define Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hYP6Umhcwci8"
      },
      "outputs": [],
      "source": [
        "class FusionLayer(nn.Module):\n",
        "    def __init__(self, image_encoder_output_size, text_encoder_output_size, hidden_size):\n",
        "        super().__init__()\n",
        "        self.attention = nn.Linear(image_encoder_output_size + text_encoder_output_size, 1)\n",
        "        self.decoder = nn.LSTM(image_encoder_output_size + text_encoder_output_size, hidden_size, num_layers=2)\n",
        "        self.linear =  nn.Linear(image_encoder_output_size + text_encoder_output_size, hidden_size)\n",
        "        self.relu = nn.ReLU()\n",
        "\n",
        "\n",
        "    # def forward(self, image_features, text_features):\n",
        "    #     # Apply average pooling to the image features\n",
        "    #     image_features = F.avg_pool1d(image_features.permute(0, 2, 1), kernel_size=2).permute(0, 2, 1)\n",
        "    #     # Combine image and text features\n",
        "    #     combined_features = torch.cat((image_features, text_features), dim=2)\n",
        "    #     # Calculate attention weights\n",
        "    #     attention_weights = torch.softmax(self.attention(combined_features), dim=1)\n",
        "\n",
        "        # Apply attention to combined features\n",
        "        # attended_features = torch.mul(combined_features, attention_weights)\n",
        "        # fused_features =self.attention(attended_features)\n",
        "        # fused_tensor_projected = self.relu(fused_features)\n",
        "        # combined_features = fused_tensor_projected.reshape(-1, 1024)\n",
        "        # # print(combined_features.shape)\n",
        "        # # vocab_max = 50258  # Update with your actual maximum vocabulary value\n",
        "        # # scaled_output = (combined_features ) * vocab_max / 2  # Scale between 0 and vocab_max\n",
        "        # # Optional: You can round values to integers if needed\n",
        "        # rounded_output = torch.round(combined_features).long()\n",
        "        # # clipped_output = torch.clamp(rounded_output, 0, vocab_max - 1)\n",
        "        # return  rounded_output\n",
        "\n",
        "    def forward(self, image_features, text_features):\n",
        "        # Apply average pooling to the image features\n",
        "        image_features = F.avg_pool1d(image_features.permute(0, 2, 1), kernel_size=2).permute(0, 2, 1)\n",
        "        # Combine image and text features\n",
        "        combined_features = torch.cat((image_features, text_features), dim=2)\n",
        "        # Calculate attention weights\n",
        "        attention_weights = torch.softmax(self.attention(combined_features), dim=1)\n",
        "\n",
        "        # Apply attention to combined features\n",
        "        attended_features = torch.mul(combined_features, attention_weights)\n",
        "        # fused_features =self.attention(attended_features)\n",
        "        fused_tensor_projected = self.linear(attended_features)\n",
        "        combined_features = fused_tensor_projected.reshape(-1, 1024)\n",
        "        # print(combined_features.shape)\n",
        "        fused_tensor_projected = torch.round(fused_tensor_projected ).long()\n",
        "        return  fused_tensor_projected\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "class GenerativeVisualDialogModel(nn.Module):\n",
        "    def __init__(self, image_encoder, dialog_encoder, question_encoder, decoder, fusion_layer):\n",
        "        super(GenerativeVisualDialogModel, self).__init__()\n",
        "        self.image_encoder = image_encoder\n",
        "        self.dialog_encoder = dialog_encoder\n",
        "        self.question_encoder = question_encoder\n",
        "        self.decoder = decoder\n",
        "        self.fusion_layer = fusion_layer\n",
        "\n",
        "        num_layers_to_freeze = int(0.8 * 768)  # Assuming 768 is the total number of layers\n",
        "        # Freeze the first layers of the image encoder\n",
        "        num_layers_frozen_image = 0\n",
        "        for child in self.image_encoder.children():\n",
        "            for param in child.parameters():\n",
        "                if num_layers_frozen_image < num_layers_to_freeze:\n",
        "                    param.requires_grad = False\n",
        "                    num_layers_frozen_image += 1\n",
        "                else:\n",
        "                    break\n",
        "        # Freeze the first layers of the dialog encoder\n",
        "        num_layers_to_freeze = int(0.8 * 1024)\n",
        "        num_layers_frozen_dialog = 0\n",
        "        for child in self.dialog_encoder.children():\n",
        "            for param in child.parameters():\n",
        "                if num_layers_frozen_dialog < num_layers_to_freeze:\n",
        "                    param.requires_grad = False\n",
        "                    num_layers_frozen_dialog += 1\n",
        "                else:\n",
        "                    break\n",
        "        num_layers_to_freeze = int(0.8 * 1024)\n",
        "        num_layers_frozen_dialog = 0\n",
        "        for child in self.question_encoder.children():\n",
        "                    for param in child.parameters():\n",
        "                        if num_layers_frozen_dialog < num_layers_to_freeze:\n",
        "                            param.requires_grad = False\n",
        "                            num_layers_frozen_dialog += 1\n",
        "                        else:\n",
        "                            break\n",
        "\n",
        "\n",
        "        num_layers_to_freeze = int(0.8 * 1024)\n",
        "        num_layers_frozen_decoder = 0\n",
        "        for child in self.decoder.children():\n",
        "            for param in child.parameters():\n",
        "                if num_layers_frozen_decoder < num_layers_to_freeze:\n",
        "                    param.requires_grad = False\n",
        "                    num_layers_frozen_dialog += 1\n",
        "                else:\n",
        "                    break\n",
        "\n",
        "    def forward(self, images, questiondialogs,answersdialogs, questions, answers):\n",
        "        # Encode images, dialogs, and questions\n",
        "        val_probs = 0\n",
        "        output = None\n",
        "        outputs = self.image_encoder(**images)\n",
        "        image_features =outputs.last_hidden_state\n",
        "        dialog_features_questions = self.dialog_encoder(**questiondialogs)\n",
        "        # dialog_features_answers = self.dialog_encoder(**answersdialogs)\n",
        "        question_features = self.question_encoder(**questions)\n",
        "        # print(dialog_features_questions.hidden_states[-2].shape,dialog_features_questions.hidden_states[-1].shape, dialog_features_questions.hidden_states[0].shape)\n",
        "        text_features = torch.cat((dialog_features_questions.hidden_states[-2], question_features.hidden_states[-2]), dim=2)\n",
        "        fused_features = self.fusion_layer( image_features, text_features)\n",
        "        # Decode the fused features using the DialogGPT decoder\n",
        "\n",
        "        # output = self.decoder.generate(input_ids = fused_features.to(torch.int32),max_length=50,pad_token_id=tokenizer.eos_token_id )\n",
        "        # output = dialog_features_questions\n",
        "       # inputs = torch.cat((torch.ones((fused_features.size(0), 1), dtype=torch.long).to(device) * tokenizer.bos_token_id, fused_features), dim=1)\n",
        "        if(answers != None):\n",
        "\n",
        "          labels = answers.reshape(-1, answers.shape[2])\n",
        "          output = self.decoder(inputs_embeds =fused_features, labels=labels)\n",
        "          # scores = output.logits\n",
        "        if(answers is None):\n",
        "            output = self.decoder.generate(inputs_embeds =fused_features, max_length=1024, pad_token_id=tokenizer.eos_token_id)\n",
        "\n",
        "        # val_probs = torch.softmax(output.logits, dim=-1)[:, :, 1]\n",
        "\n",
        "        return output,val_probs\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "EHAj5u4pfNcS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qkTwAuJr9mwI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bde8d541-10d6-40a1-e49b-25ac5c856ceb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_token.py:88: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n",
            "Some weights of Pix2StructVisionModel were not initialized from the model checkpoint at google/pix2struct-textcaps-base and are newly initialized: ['embeddings.column_embedder.weight', 'embeddings.patch_projection.bias', 'embeddings.patch_projection.weight', 'embeddings.row_embedder.weight', 'encoder.layer.0.attention.key.weight', 'encoder.layer.0.attention.output.weight', 'encoder.layer.0.attention.query.weight', 'encoder.layer.0.attention.value.weight', 'encoder.layer.0.mlp.wi_0.weight', 'encoder.layer.0.mlp.wi_1.weight', 'encoder.layer.0.mlp.wo.weight', 'encoder.layer.0.pre_attention_layer_norm.weight', 'encoder.layer.0.pre_mlp_layer_norm.weight', 'encoder.layer.1.attention.key.weight', 'encoder.layer.1.attention.output.weight', 'encoder.layer.1.attention.query.weight', 'encoder.layer.1.attention.value.weight', 'encoder.layer.1.mlp.wi_0.weight', 'encoder.layer.1.mlp.wi_1.weight', 'encoder.layer.1.mlp.wo.weight', 'encoder.layer.1.pre_attention_layer_norm.weight', 'encoder.layer.1.pre_mlp_layer_norm.weight', 'encoder.layer.10.attention.key.weight', 'encoder.layer.10.attention.output.weight', 'encoder.layer.10.attention.query.weight', 'encoder.layer.10.attention.value.weight', 'encoder.layer.10.mlp.wi_0.weight', 'encoder.layer.10.mlp.wi_1.weight', 'encoder.layer.10.mlp.wo.weight', 'encoder.layer.10.pre_attention_layer_norm.weight', 'encoder.layer.10.pre_mlp_layer_norm.weight', 'encoder.layer.11.attention.key.weight', 'encoder.layer.11.attention.output.weight', 'encoder.layer.11.attention.query.weight', 'encoder.layer.11.attention.value.weight', 'encoder.layer.11.mlp.wi_0.weight', 'encoder.layer.11.mlp.wi_1.weight', 'encoder.layer.11.mlp.wo.weight', 'encoder.layer.11.pre_attention_layer_norm.weight', 'encoder.layer.11.pre_mlp_layer_norm.weight', 'encoder.layer.2.attention.key.weight', 'encoder.layer.2.attention.output.weight', 'encoder.layer.2.attention.query.weight', 'encoder.layer.2.attention.value.weight', 'encoder.layer.2.mlp.wi_0.weight', 'encoder.layer.2.mlp.wi_1.weight', 'encoder.layer.2.mlp.wo.weight', 'encoder.layer.2.pre_attention_layer_norm.weight', 'encoder.layer.2.pre_mlp_layer_norm.weight', 'encoder.layer.3.attention.key.weight', 'encoder.layer.3.attention.output.weight', 'encoder.layer.3.attention.query.weight', 'encoder.layer.3.attention.value.weight', 'encoder.layer.3.mlp.wi_0.weight', 'encoder.layer.3.mlp.wi_1.weight', 'encoder.layer.3.mlp.wo.weight', 'encoder.layer.3.pre_attention_layer_norm.weight', 'encoder.layer.3.pre_mlp_layer_norm.weight', 'encoder.layer.4.attention.key.weight', 'encoder.layer.4.attention.output.weight', 'encoder.layer.4.attention.query.weight', 'encoder.layer.4.attention.value.weight', 'encoder.layer.4.mlp.wi_0.weight', 'encoder.layer.4.mlp.wi_1.weight', 'encoder.layer.4.mlp.wo.weight', 'encoder.layer.4.pre_attention_layer_norm.weight', 'encoder.layer.4.pre_mlp_layer_norm.weight', 'encoder.layer.5.attention.key.weight', 'encoder.layer.5.attention.output.weight', 'encoder.layer.5.attention.query.weight', 'encoder.layer.5.attention.value.weight', 'encoder.layer.5.mlp.wi_0.weight', 'encoder.layer.5.mlp.wi_1.weight', 'encoder.layer.5.mlp.wo.weight', 'encoder.layer.5.pre_attention_layer_norm.weight', 'encoder.layer.5.pre_mlp_layer_norm.weight', 'encoder.layer.6.attention.key.weight', 'encoder.layer.6.attention.output.weight', 'encoder.layer.6.attention.query.weight', 'encoder.layer.6.attention.value.weight', 'encoder.layer.6.mlp.wi_0.weight', 'encoder.layer.6.mlp.wi_1.weight', 'encoder.layer.6.mlp.wo.weight', 'encoder.layer.6.pre_attention_layer_norm.weight', 'encoder.layer.6.pre_mlp_layer_norm.weight', 'encoder.layer.7.attention.key.weight', 'encoder.layer.7.attention.output.weight', 'encoder.layer.7.attention.query.weight', 'encoder.layer.7.attention.value.weight', 'encoder.layer.7.mlp.wi_0.weight', 'encoder.layer.7.mlp.wi_1.weight', 'encoder.layer.7.mlp.wo.weight', 'encoder.layer.7.pre_attention_layer_norm.weight', 'encoder.layer.7.pre_mlp_layer_norm.weight', 'encoder.layer.8.attention.key.weight', 'encoder.layer.8.attention.output.weight', 'encoder.layer.8.attention.query.weight', 'encoder.layer.8.attention.value.weight', 'encoder.layer.8.mlp.wi_0.weight', 'encoder.layer.8.mlp.wi_1.weight', 'encoder.layer.8.mlp.wo.weight', 'encoder.layer.8.pre_attention_layer_norm.weight', 'encoder.layer.8.pre_mlp_layer_norm.weight', 'encoder.layer.9.attention.key.weight', 'encoder.layer.9.attention.output.weight', 'encoder.layer.9.attention.query.weight', 'encoder.layer.9.attention.value.weight', 'encoder.layer.9.mlp.wi_0.weight', 'encoder.layer.9.mlp.wi_1.weight', 'encoder.layer.9.mlp.wo.weight', 'encoder.layer.9.pre_attention_layer_norm.weight', 'encoder.layer.9.pre_mlp_layer_norm.weight', 'layernorm.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        }
      ],
      "source": [
        "image_encoder = Pix2StructVisionModel.from_pretrained(\"google/pix2struct-textcaps-base\")\n",
        "text_LLM_encoder = AutoModelForCausalLM.from_pretrained( \"/content/drive/MyDrive/Model/\")\n",
        "text_LLM_decoder = AutoModelForCausalLM.from_pretrained( \"/content/drive/MyDrive/Model/\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uvSuYGMUfzeT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f673ec99-891e-4f0e-d1ff-9a989995c30f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "True\n"
          ]
        }
      ],
      "source": [
        "text_LLM_encoder.config.output_hidden_states =True\n",
        "# text_LLM_decoder.config.output_hidden_states =True\n",
        "print(text_LLM_encoder.config.output_hidden_states)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Soz0VBIUx3DX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "20a13e81-683f-4e88-d93f-ab860c1fef8c"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "GPT2Config {\n",
              "  \"_name_or_path\": \"/content/drive/MyDrive/Model/\",\n",
              "  \"activation_function\": \"gelu_new\",\n",
              "  \"architectures\": [\n",
              "    \"GPT2LMHeadModel\"\n",
              "  ],\n",
              "  \"attn_pdrop\": 0.1,\n",
              "  \"bos_token_id\": 50256,\n",
              "  \"embd_pdrop\": 0.1,\n",
              "  \"eos_token_id\": 50256,\n",
              "  \"initializer_range\": 0.02,\n",
              "  \"layer_norm_epsilon\": 1e-05,\n",
              "  \"model_type\": \"gpt2\",\n",
              "  \"n_ctx\": 1024,\n",
              "  \"n_embd\": 1024,\n",
              "  \"n_head\": 16,\n",
              "  \"n_inner\": null,\n",
              "  \"n_layer\": 24,\n",
              "  \"n_positions\": 1024,\n",
              "  \"output_hidden_states\": true,\n",
              "  \"reorder_and_upcast_attn\": false,\n",
              "  \"resid_pdrop\": 0.1,\n",
              "  \"scale_attn_by_inverse_layer_idx\": false,\n",
              "  \"scale_attn_weights\": true,\n",
              "  \"summary_activation\": null,\n",
              "  \"summary_first_dropout\": 0.1,\n",
              "  \"summary_proj_to_labels\": true,\n",
              "  \"summary_type\": \"cls_index\",\n",
              "  \"summary_use_proj\": true,\n",
              "  \"task_specific_params\": {\n",
              "    \"conversational\": {\n",
              "      \"max_length\": 1000\n",
              "    }\n",
              "  },\n",
              "  \"torch_dtype\": \"float32\",\n",
              "  \"transformers_version\": \"4.37.2\",\n",
              "  \"use_cache\": true,\n",
              "  \"vocab_size\": 50258\n",
              "}"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ],
      "source": [
        "text_LLM_encoder.config"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zjjr8jplE_uA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "95ca2f07-b50b-42eb-a232-aea9b184917c"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Embedding(50258, 1024)"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ],
      "source": [
        "tokenizer = AutoTokenizer.from_pretrained('microsoft/DialoGPT-medium')\n",
        "tokenizer.add_special_tokens({'pad_token': '[PAD]'})  # Add a padding token\n",
        "#  if tokenizer.pad_token is None:\n",
        "# # Load tokenizer\n",
        "# tokenizer = AutoTokenizer.from_pretrained('microsoft/DialoGPT-medium')\n",
        "# tokenizer.add_special_tokens({'pad_token': '[PAD]'})  # Add a padding token\n",
        "image_processor = AutoProcessor.from_pretrained(\"google/pix2struct-textcaps-base\")\n",
        "text_LLM_encoder.resize_token_embeddings(len(tokenizer))\n",
        "text_LLM_decoder.resize_token_embeddings(len(tokenizer))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9pqW81UDEsmt",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d684093c-c6f8-4c60-c559-e48cafdfcd0f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "\n",
        "def download_and_save_image(url):\n",
        "    try:\n",
        "        response = requests.get(url)\n",
        "        if response.status_code == 200:\n",
        "            image = Image.open(BytesIO(response.content))\n",
        "            return image\n",
        "        else:\n",
        "            return None\n",
        "    except Exception as e:\n",
        "        print(f\"Error downloading image: {e}\")\n",
        "        return None\n",
        "\n",
        "\n",
        "def simple_preprocess(text):\n",
        "\n",
        "    tokens = word_tokenize(text)\n",
        "    tokens = [word for word in tokens if word.isalpha()]\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "    tokens = [word for word in tokens if word.lower() not in stop_words]\n",
        "    tokens = [word.lower() for word in tokens]\n",
        "    processed_text = ' '.join(tokens)\n",
        "    return processed_text\n",
        "\n",
        "\n",
        "def flatten_list(nested_list):\n",
        "    flattened = []\n",
        "    for item in nested_list:\n",
        "        if isinstance(item, list):\n",
        "            flattened.extend(flatten_list(item))\n",
        "        else:\n",
        "            flattened.append(item)\n",
        "    return flattened\n",
        "\n",
        "def concat_nested_list(nested_list):\n",
        "    if(nested_list == None):\n",
        "      return \"\"\n",
        "    flattened = flatten_list(nested_list)\n",
        "    return ''.join(map(str, flattened))\n",
        "\n",
        "def find_longest_sublist(main_list):\n",
        "    longest_sublist = []\n",
        "    for sublist in main_list:\n",
        "        if len(sublist) > len(longest_sublist):\n",
        "            longest_sublist = sublist\n",
        "    return longest_sublist\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3CweFy04nCKn"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoTokenizer\n",
        "import torch\n",
        "from PIL import Image\n",
        "from transformers import AutoProcessor, Pix2StructVisionModel\n",
        "\n",
        "# Define function to preprocess data\n",
        "def preprocess_data(Q_dialog, A_dialog, Question ,image_path, answer):\n",
        "    # Tokenize dialog\n",
        "    Qdialog_tokens = tokenizer(Q_dialog, padding=\"max_length\", truncation=True, max_length=1024, return_tensors=\"pt\")\n",
        "    Adialog_tokens = tokenizer( A_dialog, padding=\"max_length\", truncation=True, max_length=1024, return_tensors=\"pt\")\n",
        "    # Load and process image\n",
        "    image = Image.open(requests.get(image_path, stream=True).raw)\n",
        "    image = image.convert(\"RGB\")\n",
        "    Question_tokens =  tokenizer(Question, padding=\"max_length\", truncation=True, max_length=1024, return_tensors=\"pt\")\n",
        "    answer_tokenized = tokenizer.encode(answer, padding=\"max_length\", truncation=True, max_length=1024, return_tensors=\"pt\")\n",
        "    return Qdialog_tokens, Adialog_tokens, Question_tokens, image, answer_tokenized\n",
        "\n",
        "# # Example usage\n",
        "# dialog_tokens, image, answer_tokenized = preprocess_data(\"Sample dialog\", \"image.jpg\", \"Sample answer\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# s1 = \"this is just for test\"\n",
        "\n",
        "\n",
        "# s = tokenizer(s1, padding=\"max_length\", truncation=True, max_length=1024, return_tensors=\"pt\").to(device)\n",
        "# M = text_LLM_encoder(**s)\n",
        "# print(M.hidden_states[0].shape,M.hidden_states[-1].shape ,M.hidden_states[-2].shape ,M.hidden_states[1].shape , len(M.hidden_states)  )"
      ],
      "metadata": {
        "id": "WaOkGBnQv2wZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DaFZYAqNE4PL"
      },
      "outputs": [],
      "source": [
        "train_val_test_split = mydataset.train_test_split(test_size=0.995, shuffle=True)\n",
        "\n",
        "# Split the trainval set into train and validation sets\n",
        "train_val_split = train_val_test_split[\"train\"].train_test_split(test_size=0.20, shuffle=True)\n",
        "\n",
        "# Assign train, validation, and test sets\n",
        "train_dataset = train_val_split[\"train\"]\n",
        "val_dataset = train_val_split[\"test\"]\n",
        "test_dataset = train_val_test_split[\"test\"]"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(train_dataset)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-0TfcRXfnx2B",
        "outputId": "483a42ca-8bf5-4248-ecf5-4267f5c632b8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset({\n",
            "    features: ['Id', 'Body', 'DialogHistory', 'Image', 'Answer', 'DialogAnswers'],\n",
            "    num_rows: 32\n",
            "})\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(val_dataset)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jsHGr_TCn0zV",
        "outputId": "3fbfb097-3f86-46a0-e189-c84393832210"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset({\n",
            "    features: ['Id', 'Body', 'DialogHistory', 'Image', 'Answer', 'DialogAnswers'],\n",
            "    num_rows: 8\n",
            "})\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(test_dataset)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-nGRYoa4n2k1",
        "outputId": "fcb45a11-ded7-45d5-f93f-f6fe2889f866"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset({\n",
            "    features: ['Id', 'Body', 'DialogHistory', 'Image', 'Answer', 'DialogAnswers'],\n",
            "    num_rows: 7984\n",
            "})\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NJA3esr3o3-5"
      },
      "outputs": [],
      "source": [
        "from torch.utils.data import Dataset, DataLoader\n",
        "class MyDataset(Dataset):\n",
        "    def __init__(self, data):\n",
        "        self.data = data\n",
        "        self.transform = transforms.Compose([\n",
        "            transforms.Resize((768, 768)),  # Resize images to a fixed size\n",
        "            transforms.ToTensor(),           # Convert images to PyTorch tensors\n",
        "        ])\n",
        "\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        # print(\"am I the problem??????????? \")\n",
        "\n",
        "        ID, Question, Q_dialog, image_path, answer, A_dialog = self.data[idx]['Id'], self.data[idx]['Body'], self.data[idx]['DialogHistory'], self.data[idx]['Image'], self.data[idx]['Answer'], self.data[idx]['DialogAnswers']\n",
        "        # print(\"am I the problem???????????????? \")\n",
        "        print(ID, \"Question:##\",Question,\"Dialogs:####\", Q_dialog,\"Image path:\" ,image_path, \"answer:#####\",answer)\n",
        "        if A_dialog is not None:\n",
        "            # print(\"gv\", A_dialog[0])\n",
        "            A_dialog =   find_longest_sublist(A_dialog)\n",
        "        if Q_dialog is not None:\n",
        "            # print(\"g\", Q_dialog[0])\n",
        "            Q_dialog=  find_longest_sublist(Q_dialog)\n",
        "\n",
        "        Q_dialog_string = concat_nested_list(Q_dialog)\n",
        "        A_dialog_string = concat_nested_list(A_dialog)\n",
        "        Qdialog_tokens, Adialog_tokens, Question_tokens, image, answer_tokenized = preprocess_data(Q_dialog_string, A_dialog_string, Question, image_path, answer)\n",
        "        image = self.transform(image)\n",
        "        # Move tensors to device\n",
        "        Qdialog_tokens = Qdialog_tokens.to(device)\n",
        "        Adialog_tokens = Adialog_tokens.to(device)\n",
        "        Question_tokens = Question_tokens.to(device)\n",
        "        answer_tokenized = answer_tokenized.to(device)\n",
        "        # image = image.to(device)\n",
        "\n",
        "        # print(image)\n",
        "        # print(image.size())\n",
        "        return Qdialog_tokens, Adialog_tokens, Question_tokens, image, answer_tokenized\n",
        "\n",
        "\n",
        "trainDataset = MyDataset(train_dataset)\n",
        "valDataset = MyDataset(val_dataset)\n",
        "# Define data loader\n",
        "batch_size = 4\n",
        "data_loader = DataLoader(trainDataset , batch_size=batch_size, shuffle=True)\n",
        "val_loader =  DataLoader(valDataset, batch_size=batch_size, shuffle=True)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_fj56vkIzSYb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d8564b6d-3a90-4386-b5b1-1606d25fdce3"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "GenerativeVisualDialogModel(\n",
              "  (image_encoder): Pix2StructVisionModel(\n",
              "    (embeddings): Pix2StructVisionEmbeddings(\n",
              "      (patch_projection): Linear(in_features=768, out_features=768, bias=True)\n",
              "      (row_embedder): Embedding(4096, 768)\n",
              "      (column_embedder): Embedding(4096, 768)\n",
              "      (dropout): Dropout(p=0.06, inplace=False)\n",
              "    )\n",
              "    (encoder): Pix2StructVisionEncoder(\n",
              "      (layer): ModuleList(\n",
              "        (0-11): 12 x Pix2StructVisionLayer(\n",
              "          (attention): Pix2StructVisionAttention(\n",
              "            (query): Linear(in_features=768, out_features=768, bias=False)\n",
              "            (key): Linear(in_features=768, out_features=768, bias=False)\n",
              "            (value): Linear(in_features=768, out_features=768, bias=False)\n",
              "            (output): Linear(in_features=768, out_features=768, bias=False)\n",
              "          )\n",
              "          (mlp): Pix2StructVisionMlp(\n",
              "            (wi_0): Linear(in_features=768, out_features=2048, bias=False)\n",
              "            (wi_1): Linear(in_features=768, out_features=2048, bias=False)\n",
              "            (wo): Linear(in_features=2048, out_features=768, bias=False)\n",
              "            (dropout): Dropout(p=0.06, inplace=False)\n",
              "            (act): NewGELUActivation()\n",
              "          )\n",
              "          (pre_mlp_layer_norm): Pix2StructLayerNorm()\n",
              "          (pre_attention_layer_norm): Pix2StructLayerNorm()\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (layernorm): Pix2StructLayerNorm()\n",
              "  )\n",
              "  (dialog_encoder): GPT2LMHeadModel(\n",
              "    (transformer): GPT2Model(\n",
              "      (wte): Embedding(50258, 1024)\n",
              "      (wpe): Embedding(1024, 1024)\n",
              "      (drop): Dropout(p=0.1, inplace=False)\n",
              "      (h): ModuleList(\n",
              "        (0-23): 24 x GPT2Block(\n",
              "          (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "          (attn): GPT2Attention(\n",
              "            (c_attn): Conv1D()\n",
              "            (c_proj): Conv1D()\n",
              "            (attn_dropout): Dropout(p=0.1, inplace=False)\n",
              "            (resid_dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "          (mlp): GPT2MLP(\n",
              "            (c_fc): Conv1D()\n",
              "            (c_proj): Conv1D()\n",
              "            (act): NewGELUActivation()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "      (ln_f): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "    )\n",
              "    (lm_head): Linear(in_features=1024, out_features=50258, bias=False)\n",
              "  )\n",
              "  (question_encoder): GPT2LMHeadModel(\n",
              "    (transformer): GPT2Model(\n",
              "      (wte): Embedding(50258, 1024)\n",
              "      (wpe): Embedding(1024, 1024)\n",
              "      (drop): Dropout(p=0.1, inplace=False)\n",
              "      (h): ModuleList(\n",
              "        (0-23): 24 x GPT2Block(\n",
              "          (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "          (attn): GPT2Attention(\n",
              "            (c_attn): Conv1D()\n",
              "            (c_proj): Conv1D()\n",
              "            (attn_dropout): Dropout(p=0.1, inplace=False)\n",
              "            (resid_dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "          (mlp): GPT2MLP(\n",
              "            (c_fc): Conv1D()\n",
              "            (c_proj): Conv1D()\n",
              "            (act): NewGELUActivation()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "      (ln_f): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "    )\n",
              "    (lm_head): Linear(in_features=1024, out_features=50258, bias=False)\n",
              "  )\n",
              "  (decoder): GPT2LMHeadModel(\n",
              "    (transformer): GPT2Model(\n",
              "      (wte): Embedding(50258, 1024)\n",
              "      (wpe): Embedding(1024, 1024)\n",
              "      (drop): Dropout(p=0.1, inplace=False)\n",
              "      (h): ModuleList(\n",
              "        (0-23): 24 x GPT2Block(\n",
              "          (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "          (attn): GPT2Attention(\n",
              "            (c_attn): Conv1D()\n",
              "            (c_proj): Conv1D()\n",
              "            (attn_dropout): Dropout(p=0.1, inplace=False)\n",
              "            (resid_dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "          (mlp): GPT2MLP(\n",
              "            (c_fc): Conv1D()\n",
              "            (c_proj): Conv1D()\n",
              "            (act): NewGELUActivation()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "      (ln_f): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "    )\n",
              "    (lm_head): Linear(in_features=1024, out_features=50258, bias=False)\n",
              "  )\n",
              "  (fusion_layer): FusionLayer(\n",
              "    (attention): Linear(in_features=2816, out_features=1, bias=True)\n",
              "    (decoder): LSTM(2816, 1024, num_layers=2)\n",
              "    (linear): Linear(in_features=2816, out_features=1024, bias=True)\n",
              "    (relu): ReLU()\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "\n",
        "# Define model, optimizer, loss function\n",
        "fusion_layer = FusionLayer(image_encoder_output_size=768, text_encoder_output_size=2048, hidden_size=text_LLM_decoder.config.n_embd)\n",
        "\n",
        "model = GenerativeVisualDialogModel(image_encoder, text_LLM_encoder, text_LLM_encoder, text_LLM_decoder, fusion_layer)\n",
        "optimizer = optim.AdamW(model.parameters(), lr=1e-4)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "model.to(device)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text_LLM_decoder.config.n_embd"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wHcV5u1P_GiO",
        "outputId": "ac670e84-0e52-4f33-9695-5f1d2ce0ca49"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1024"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "path = F\"/content/drive/MyDrive/Final_Model/FILE_NAME\""
      ],
      "metadata": {
        "id": "M2HGIS3O5Xc7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# checkpoint = torch.load(path, map_location=torch.device('cpu'))\n",
        "checkpoint = torch.load(path)\n",
        "# Print the keys to identify the correct key for the model state dictionary\n",
        "print(checkpoint.keys())\n",
        "\n",
        "# Load the model state_dict using the correct key\n",
        "model.load_state_dict(checkpoint)"
      ],
      "metadata": {
        "id": "FdQ4cd5Vgyea",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f330ba3f-d073-4797-f1f6-21f24f3ec5ac"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "odict_keys(['image_encoder.embeddings.patch_projection.weight', 'image_encoder.embeddings.patch_projection.bias', 'image_encoder.embeddings.row_embedder.weight', 'image_encoder.embeddings.column_embedder.weight', 'image_encoder.encoder.layer.0.attention.query.weight', 'image_encoder.encoder.layer.0.attention.key.weight', 'image_encoder.encoder.layer.0.attention.value.weight', 'image_encoder.encoder.layer.0.attention.output.weight', 'image_encoder.encoder.layer.0.mlp.wi_0.weight', 'image_encoder.encoder.layer.0.mlp.wi_1.weight', 'image_encoder.encoder.layer.0.mlp.wo.weight', 'image_encoder.encoder.layer.0.pre_mlp_layer_norm.weight', 'image_encoder.encoder.layer.0.pre_attention_layer_norm.weight', 'image_encoder.encoder.layer.1.attention.query.weight', 'image_encoder.encoder.layer.1.attention.key.weight', 'image_encoder.encoder.layer.1.attention.value.weight', 'image_encoder.encoder.layer.1.attention.output.weight', 'image_encoder.encoder.layer.1.mlp.wi_0.weight', 'image_encoder.encoder.layer.1.mlp.wi_1.weight', 'image_encoder.encoder.layer.1.mlp.wo.weight', 'image_encoder.encoder.layer.1.pre_mlp_layer_norm.weight', 'image_encoder.encoder.layer.1.pre_attention_layer_norm.weight', 'image_encoder.encoder.layer.2.attention.query.weight', 'image_encoder.encoder.layer.2.attention.key.weight', 'image_encoder.encoder.layer.2.attention.value.weight', 'image_encoder.encoder.layer.2.attention.output.weight', 'image_encoder.encoder.layer.2.mlp.wi_0.weight', 'image_encoder.encoder.layer.2.mlp.wi_1.weight', 'image_encoder.encoder.layer.2.mlp.wo.weight', 'image_encoder.encoder.layer.2.pre_mlp_layer_norm.weight', 'image_encoder.encoder.layer.2.pre_attention_layer_norm.weight', 'image_encoder.encoder.layer.3.attention.query.weight', 'image_encoder.encoder.layer.3.attention.key.weight', 'image_encoder.encoder.layer.3.attention.value.weight', 'image_encoder.encoder.layer.3.attention.output.weight', 'image_encoder.encoder.layer.3.mlp.wi_0.weight', 'image_encoder.encoder.layer.3.mlp.wi_1.weight', 'image_encoder.encoder.layer.3.mlp.wo.weight', 'image_encoder.encoder.layer.3.pre_mlp_layer_norm.weight', 'image_encoder.encoder.layer.3.pre_attention_layer_norm.weight', 'image_encoder.encoder.layer.4.attention.query.weight', 'image_encoder.encoder.layer.4.attention.key.weight', 'image_encoder.encoder.layer.4.attention.value.weight', 'image_encoder.encoder.layer.4.attention.output.weight', 'image_encoder.encoder.layer.4.mlp.wi_0.weight', 'image_encoder.encoder.layer.4.mlp.wi_1.weight', 'image_encoder.encoder.layer.4.mlp.wo.weight', 'image_encoder.encoder.layer.4.pre_mlp_layer_norm.weight', 'image_encoder.encoder.layer.4.pre_attention_layer_norm.weight', 'image_encoder.encoder.layer.5.attention.query.weight', 'image_encoder.encoder.layer.5.attention.key.weight', 'image_encoder.encoder.layer.5.attention.value.weight', 'image_encoder.encoder.layer.5.attention.output.weight', 'image_encoder.encoder.layer.5.mlp.wi_0.weight', 'image_encoder.encoder.layer.5.mlp.wi_1.weight', 'image_encoder.encoder.layer.5.mlp.wo.weight', 'image_encoder.encoder.layer.5.pre_mlp_layer_norm.weight', 'image_encoder.encoder.layer.5.pre_attention_layer_norm.weight', 'image_encoder.encoder.layer.6.attention.query.weight', 'image_encoder.encoder.layer.6.attention.key.weight', 'image_encoder.encoder.layer.6.attention.value.weight', 'image_encoder.encoder.layer.6.attention.output.weight', 'image_encoder.encoder.layer.6.mlp.wi_0.weight', 'image_encoder.encoder.layer.6.mlp.wi_1.weight', 'image_encoder.encoder.layer.6.mlp.wo.weight', 'image_encoder.encoder.layer.6.pre_mlp_layer_norm.weight', 'image_encoder.encoder.layer.6.pre_attention_layer_norm.weight', 'image_encoder.encoder.layer.7.attention.query.weight', 'image_encoder.encoder.layer.7.attention.key.weight', 'image_encoder.encoder.layer.7.attention.value.weight', 'image_encoder.encoder.layer.7.attention.output.weight', 'image_encoder.encoder.layer.7.mlp.wi_0.weight', 'image_encoder.encoder.layer.7.mlp.wi_1.weight', 'image_encoder.encoder.layer.7.mlp.wo.weight', 'image_encoder.encoder.layer.7.pre_mlp_layer_norm.weight', 'image_encoder.encoder.layer.7.pre_attention_layer_norm.weight', 'image_encoder.encoder.layer.8.attention.query.weight', 'image_encoder.encoder.layer.8.attention.key.weight', 'image_encoder.encoder.layer.8.attention.value.weight', 'image_encoder.encoder.layer.8.attention.output.weight', 'image_encoder.encoder.layer.8.mlp.wi_0.weight', 'image_encoder.encoder.layer.8.mlp.wi_1.weight', 'image_encoder.encoder.layer.8.mlp.wo.weight', 'image_encoder.encoder.layer.8.pre_mlp_layer_norm.weight', 'image_encoder.encoder.layer.8.pre_attention_layer_norm.weight', 'image_encoder.encoder.layer.9.attention.query.weight', 'image_encoder.encoder.layer.9.attention.key.weight', 'image_encoder.encoder.layer.9.attention.value.weight', 'image_encoder.encoder.layer.9.attention.output.weight', 'image_encoder.encoder.layer.9.mlp.wi_0.weight', 'image_encoder.encoder.layer.9.mlp.wi_1.weight', 'image_encoder.encoder.layer.9.mlp.wo.weight', 'image_encoder.encoder.layer.9.pre_mlp_layer_norm.weight', 'image_encoder.encoder.layer.9.pre_attention_layer_norm.weight', 'image_encoder.encoder.layer.10.attention.query.weight', 'image_encoder.encoder.layer.10.attention.key.weight', 'image_encoder.encoder.layer.10.attention.value.weight', 'image_encoder.encoder.layer.10.attention.output.weight', 'image_encoder.encoder.layer.10.mlp.wi_0.weight', 'image_encoder.encoder.layer.10.mlp.wi_1.weight', 'image_encoder.encoder.layer.10.mlp.wo.weight', 'image_encoder.encoder.layer.10.pre_mlp_layer_norm.weight', 'image_encoder.encoder.layer.10.pre_attention_layer_norm.weight', 'image_encoder.encoder.layer.11.attention.query.weight', 'image_encoder.encoder.layer.11.attention.key.weight', 'image_encoder.encoder.layer.11.attention.value.weight', 'image_encoder.encoder.layer.11.attention.output.weight', 'image_encoder.encoder.layer.11.mlp.wi_0.weight', 'image_encoder.encoder.layer.11.mlp.wi_1.weight', 'image_encoder.encoder.layer.11.mlp.wo.weight', 'image_encoder.encoder.layer.11.pre_mlp_layer_norm.weight', 'image_encoder.encoder.layer.11.pre_attention_layer_norm.weight', 'image_encoder.layernorm.weight', 'dialog_encoder.transformer.wte.weight', 'dialog_encoder.transformer.wpe.weight', 'dialog_encoder.transformer.h.0.ln_1.weight', 'dialog_encoder.transformer.h.0.ln_1.bias', 'dialog_encoder.transformer.h.0.attn.c_attn.weight', 'dialog_encoder.transformer.h.0.attn.c_attn.bias', 'dialog_encoder.transformer.h.0.attn.c_proj.weight', 'dialog_encoder.transformer.h.0.attn.c_proj.bias', 'dialog_encoder.transformer.h.0.ln_2.weight', 'dialog_encoder.transformer.h.0.ln_2.bias', 'dialog_encoder.transformer.h.0.mlp.c_fc.weight', 'dialog_encoder.transformer.h.0.mlp.c_fc.bias', 'dialog_encoder.transformer.h.0.mlp.c_proj.weight', 'dialog_encoder.transformer.h.0.mlp.c_proj.bias', 'dialog_encoder.transformer.h.1.ln_1.weight', 'dialog_encoder.transformer.h.1.ln_1.bias', 'dialog_encoder.transformer.h.1.attn.c_attn.weight', 'dialog_encoder.transformer.h.1.attn.c_attn.bias', 'dialog_encoder.transformer.h.1.attn.c_proj.weight', 'dialog_encoder.transformer.h.1.attn.c_proj.bias', 'dialog_encoder.transformer.h.1.ln_2.weight', 'dialog_encoder.transformer.h.1.ln_2.bias', 'dialog_encoder.transformer.h.1.mlp.c_fc.weight', 'dialog_encoder.transformer.h.1.mlp.c_fc.bias', 'dialog_encoder.transformer.h.1.mlp.c_proj.weight', 'dialog_encoder.transformer.h.1.mlp.c_proj.bias', 'dialog_encoder.transformer.h.2.ln_1.weight', 'dialog_encoder.transformer.h.2.ln_1.bias', 'dialog_encoder.transformer.h.2.attn.c_attn.weight', 'dialog_encoder.transformer.h.2.attn.c_attn.bias', 'dialog_encoder.transformer.h.2.attn.c_proj.weight', 'dialog_encoder.transformer.h.2.attn.c_proj.bias', 'dialog_encoder.transformer.h.2.ln_2.weight', 'dialog_encoder.transformer.h.2.ln_2.bias', 'dialog_encoder.transformer.h.2.mlp.c_fc.weight', 'dialog_encoder.transformer.h.2.mlp.c_fc.bias', 'dialog_encoder.transformer.h.2.mlp.c_proj.weight', 'dialog_encoder.transformer.h.2.mlp.c_proj.bias', 'dialog_encoder.transformer.h.3.ln_1.weight', 'dialog_encoder.transformer.h.3.ln_1.bias', 'dialog_encoder.transformer.h.3.attn.c_attn.weight', 'dialog_encoder.transformer.h.3.attn.c_attn.bias', 'dialog_encoder.transformer.h.3.attn.c_proj.weight', 'dialog_encoder.transformer.h.3.attn.c_proj.bias', 'dialog_encoder.transformer.h.3.ln_2.weight', 'dialog_encoder.transformer.h.3.ln_2.bias', 'dialog_encoder.transformer.h.3.mlp.c_fc.weight', 'dialog_encoder.transformer.h.3.mlp.c_fc.bias', 'dialog_encoder.transformer.h.3.mlp.c_proj.weight', 'dialog_encoder.transformer.h.3.mlp.c_proj.bias', 'dialog_encoder.transformer.h.4.ln_1.weight', 'dialog_encoder.transformer.h.4.ln_1.bias', 'dialog_encoder.transformer.h.4.attn.c_attn.weight', 'dialog_encoder.transformer.h.4.attn.c_attn.bias', 'dialog_encoder.transformer.h.4.attn.c_proj.weight', 'dialog_encoder.transformer.h.4.attn.c_proj.bias', 'dialog_encoder.transformer.h.4.ln_2.weight', 'dialog_encoder.transformer.h.4.ln_2.bias', 'dialog_encoder.transformer.h.4.mlp.c_fc.weight', 'dialog_encoder.transformer.h.4.mlp.c_fc.bias', 'dialog_encoder.transformer.h.4.mlp.c_proj.weight', 'dialog_encoder.transformer.h.4.mlp.c_proj.bias', 'dialog_encoder.transformer.h.5.ln_1.weight', 'dialog_encoder.transformer.h.5.ln_1.bias', 'dialog_encoder.transformer.h.5.attn.c_attn.weight', 'dialog_encoder.transformer.h.5.attn.c_attn.bias', 'dialog_encoder.transformer.h.5.attn.c_proj.weight', 'dialog_encoder.transformer.h.5.attn.c_proj.bias', 'dialog_encoder.transformer.h.5.ln_2.weight', 'dialog_encoder.transformer.h.5.ln_2.bias', 'dialog_encoder.transformer.h.5.mlp.c_fc.weight', 'dialog_encoder.transformer.h.5.mlp.c_fc.bias', 'dialog_encoder.transformer.h.5.mlp.c_proj.weight', 'dialog_encoder.transformer.h.5.mlp.c_proj.bias', 'dialog_encoder.transformer.h.6.ln_1.weight', 'dialog_encoder.transformer.h.6.ln_1.bias', 'dialog_encoder.transformer.h.6.attn.c_attn.weight', 'dialog_encoder.transformer.h.6.attn.c_attn.bias', 'dialog_encoder.transformer.h.6.attn.c_proj.weight', 'dialog_encoder.transformer.h.6.attn.c_proj.bias', 'dialog_encoder.transformer.h.6.ln_2.weight', 'dialog_encoder.transformer.h.6.ln_2.bias', 'dialog_encoder.transformer.h.6.mlp.c_fc.weight', 'dialog_encoder.transformer.h.6.mlp.c_fc.bias', 'dialog_encoder.transformer.h.6.mlp.c_proj.weight', 'dialog_encoder.transformer.h.6.mlp.c_proj.bias', 'dialog_encoder.transformer.h.7.ln_1.weight', 'dialog_encoder.transformer.h.7.ln_1.bias', 'dialog_encoder.transformer.h.7.attn.c_attn.weight', 'dialog_encoder.transformer.h.7.attn.c_attn.bias', 'dialog_encoder.transformer.h.7.attn.c_proj.weight', 'dialog_encoder.transformer.h.7.attn.c_proj.bias', 'dialog_encoder.transformer.h.7.ln_2.weight', 'dialog_encoder.transformer.h.7.ln_2.bias', 'dialog_encoder.transformer.h.7.mlp.c_fc.weight', 'dialog_encoder.transformer.h.7.mlp.c_fc.bias', 'dialog_encoder.transformer.h.7.mlp.c_proj.weight', 'dialog_encoder.transformer.h.7.mlp.c_proj.bias', 'dialog_encoder.transformer.h.8.ln_1.weight', 'dialog_encoder.transformer.h.8.ln_1.bias', 'dialog_encoder.transformer.h.8.attn.c_attn.weight', 'dialog_encoder.transformer.h.8.attn.c_attn.bias', 'dialog_encoder.transformer.h.8.attn.c_proj.weight', 'dialog_encoder.transformer.h.8.attn.c_proj.bias', 'dialog_encoder.transformer.h.8.ln_2.weight', 'dialog_encoder.transformer.h.8.ln_2.bias', 'dialog_encoder.transformer.h.8.mlp.c_fc.weight', 'dialog_encoder.transformer.h.8.mlp.c_fc.bias', 'dialog_encoder.transformer.h.8.mlp.c_proj.weight', 'dialog_encoder.transformer.h.8.mlp.c_proj.bias', 'dialog_encoder.transformer.h.9.ln_1.weight', 'dialog_encoder.transformer.h.9.ln_1.bias', 'dialog_encoder.transformer.h.9.attn.c_attn.weight', 'dialog_encoder.transformer.h.9.attn.c_attn.bias', 'dialog_encoder.transformer.h.9.attn.c_proj.weight', 'dialog_encoder.transformer.h.9.attn.c_proj.bias', 'dialog_encoder.transformer.h.9.ln_2.weight', 'dialog_encoder.transformer.h.9.ln_2.bias', 'dialog_encoder.transformer.h.9.mlp.c_fc.weight', 'dialog_encoder.transformer.h.9.mlp.c_fc.bias', 'dialog_encoder.transformer.h.9.mlp.c_proj.weight', 'dialog_encoder.transformer.h.9.mlp.c_proj.bias', 'dialog_encoder.transformer.h.10.ln_1.weight', 'dialog_encoder.transformer.h.10.ln_1.bias', 'dialog_encoder.transformer.h.10.attn.c_attn.weight', 'dialog_encoder.transformer.h.10.attn.c_attn.bias', 'dialog_encoder.transformer.h.10.attn.c_proj.weight', 'dialog_encoder.transformer.h.10.attn.c_proj.bias', 'dialog_encoder.transformer.h.10.ln_2.weight', 'dialog_encoder.transformer.h.10.ln_2.bias', 'dialog_encoder.transformer.h.10.mlp.c_fc.weight', 'dialog_encoder.transformer.h.10.mlp.c_fc.bias', 'dialog_encoder.transformer.h.10.mlp.c_proj.weight', 'dialog_encoder.transformer.h.10.mlp.c_proj.bias', 'dialog_encoder.transformer.h.11.ln_1.weight', 'dialog_encoder.transformer.h.11.ln_1.bias', 'dialog_encoder.transformer.h.11.attn.c_attn.weight', 'dialog_encoder.transformer.h.11.attn.c_attn.bias', 'dialog_encoder.transformer.h.11.attn.c_proj.weight', 'dialog_encoder.transformer.h.11.attn.c_proj.bias', 'dialog_encoder.transformer.h.11.ln_2.weight', 'dialog_encoder.transformer.h.11.ln_2.bias', 'dialog_encoder.transformer.h.11.mlp.c_fc.weight', 'dialog_encoder.transformer.h.11.mlp.c_fc.bias', 'dialog_encoder.transformer.h.11.mlp.c_proj.weight', 'dialog_encoder.transformer.h.11.mlp.c_proj.bias', 'dialog_encoder.transformer.h.12.ln_1.weight', 'dialog_encoder.transformer.h.12.ln_1.bias', 'dialog_encoder.transformer.h.12.attn.c_attn.weight', 'dialog_encoder.transformer.h.12.attn.c_attn.bias', 'dialog_encoder.transformer.h.12.attn.c_proj.weight', 'dialog_encoder.transformer.h.12.attn.c_proj.bias', 'dialog_encoder.transformer.h.12.ln_2.weight', 'dialog_encoder.transformer.h.12.ln_2.bias', 'dialog_encoder.transformer.h.12.mlp.c_fc.weight', 'dialog_encoder.transformer.h.12.mlp.c_fc.bias', 'dialog_encoder.transformer.h.12.mlp.c_proj.weight', 'dialog_encoder.transformer.h.12.mlp.c_proj.bias', 'dialog_encoder.transformer.h.13.ln_1.weight', 'dialog_encoder.transformer.h.13.ln_1.bias', 'dialog_encoder.transformer.h.13.attn.c_attn.weight', 'dialog_encoder.transformer.h.13.attn.c_attn.bias', 'dialog_encoder.transformer.h.13.attn.c_proj.weight', 'dialog_encoder.transformer.h.13.attn.c_proj.bias', 'dialog_encoder.transformer.h.13.ln_2.weight', 'dialog_encoder.transformer.h.13.ln_2.bias', 'dialog_encoder.transformer.h.13.mlp.c_fc.weight', 'dialog_encoder.transformer.h.13.mlp.c_fc.bias', 'dialog_encoder.transformer.h.13.mlp.c_proj.weight', 'dialog_encoder.transformer.h.13.mlp.c_proj.bias', 'dialog_encoder.transformer.h.14.ln_1.weight', 'dialog_encoder.transformer.h.14.ln_1.bias', 'dialog_encoder.transformer.h.14.attn.c_attn.weight', 'dialog_encoder.transformer.h.14.attn.c_attn.bias', 'dialog_encoder.transformer.h.14.attn.c_proj.weight', 'dialog_encoder.transformer.h.14.attn.c_proj.bias', 'dialog_encoder.transformer.h.14.ln_2.weight', 'dialog_encoder.transformer.h.14.ln_2.bias', 'dialog_encoder.transformer.h.14.mlp.c_fc.weight', 'dialog_encoder.transformer.h.14.mlp.c_fc.bias', 'dialog_encoder.transformer.h.14.mlp.c_proj.weight', 'dialog_encoder.transformer.h.14.mlp.c_proj.bias', 'dialog_encoder.transformer.h.15.ln_1.weight', 'dialog_encoder.transformer.h.15.ln_1.bias', 'dialog_encoder.transformer.h.15.attn.c_attn.weight', 'dialog_encoder.transformer.h.15.attn.c_attn.bias', 'dialog_encoder.transformer.h.15.attn.c_proj.weight', 'dialog_encoder.transformer.h.15.attn.c_proj.bias', 'dialog_encoder.transformer.h.15.ln_2.weight', 'dialog_encoder.transformer.h.15.ln_2.bias', 'dialog_encoder.transformer.h.15.mlp.c_fc.weight', 'dialog_encoder.transformer.h.15.mlp.c_fc.bias', 'dialog_encoder.transformer.h.15.mlp.c_proj.weight', 'dialog_encoder.transformer.h.15.mlp.c_proj.bias', 'dialog_encoder.transformer.h.16.ln_1.weight', 'dialog_encoder.transformer.h.16.ln_1.bias', 'dialog_encoder.transformer.h.16.attn.c_attn.weight', 'dialog_encoder.transformer.h.16.attn.c_attn.bias', 'dialog_encoder.transformer.h.16.attn.c_proj.weight', 'dialog_encoder.transformer.h.16.attn.c_proj.bias', 'dialog_encoder.transformer.h.16.ln_2.weight', 'dialog_encoder.transformer.h.16.ln_2.bias', 'dialog_encoder.transformer.h.16.mlp.c_fc.weight', 'dialog_encoder.transformer.h.16.mlp.c_fc.bias', 'dialog_encoder.transformer.h.16.mlp.c_proj.weight', 'dialog_encoder.transformer.h.16.mlp.c_proj.bias', 'dialog_encoder.transformer.h.17.ln_1.weight', 'dialog_encoder.transformer.h.17.ln_1.bias', 'dialog_encoder.transformer.h.17.attn.c_attn.weight', 'dialog_encoder.transformer.h.17.attn.c_attn.bias', 'dialog_encoder.transformer.h.17.attn.c_proj.weight', 'dialog_encoder.transformer.h.17.attn.c_proj.bias', 'dialog_encoder.transformer.h.17.ln_2.weight', 'dialog_encoder.transformer.h.17.ln_2.bias', 'dialog_encoder.transformer.h.17.mlp.c_fc.weight', 'dialog_encoder.transformer.h.17.mlp.c_fc.bias', 'dialog_encoder.transformer.h.17.mlp.c_proj.weight', 'dialog_encoder.transformer.h.17.mlp.c_proj.bias', 'dialog_encoder.transformer.h.18.ln_1.weight', 'dialog_encoder.transformer.h.18.ln_1.bias', 'dialog_encoder.transformer.h.18.attn.c_attn.weight', 'dialog_encoder.transformer.h.18.attn.c_attn.bias', 'dialog_encoder.transformer.h.18.attn.c_proj.weight', 'dialog_encoder.transformer.h.18.attn.c_proj.bias', 'dialog_encoder.transformer.h.18.ln_2.weight', 'dialog_encoder.transformer.h.18.ln_2.bias', 'dialog_encoder.transformer.h.18.mlp.c_fc.weight', 'dialog_encoder.transformer.h.18.mlp.c_fc.bias', 'dialog_encoder.transformer.h.18.mlp.c_proj.weight', 'dialog_encoder.transformer.h.18.mlp.c_proj.bias', 'dialog_encoder.transformer.h.19.ln_1.weight', 'dialog_encoder.transformer.h.19.ln_1.bias', 'dialog_encoder.transformer.h.19.attn.c_attn.weight', 'dialog_encoder.transformer.h.19.attn.c_attn.bias', 'dialog_encoder.transformer.h.19.attn.c_proj.weight', 'dialog_encoder.transformer.h.19.attn.c_proj.bias', 'dialog_encoder.transformer.h.19.ln_2.weight', 'dialog_encoder.transformer.h.19.ln_2.bias', 'dialog_encoder.transformer.h.19.mlp.c_fc.weight', 'dialog_encoder.transformer.h.19.mlp.c_fc.bias', 'dialog_encoder.transformer.h.19.mlp.c_proj.weight', 'dialog_encoder.transformer.h.19.mlp.c_proj.bias', 'dialog_encoder.transformer.h.20.ln_1.weight', 'dialog_encoder.transformer.h.20.ln_1.bias', 'dialog_encoder.transformer.h.20.attn.c_attn.weight', 'dialog_encoder.transformer.h.20.attn.c_attn.bias', 'dialog_encoder.transformer.h.20.attn.c_proj.weight', 'dialog_encoder.transformer.h.20.attn.c_proj.bias', 'dialog_encoder.transformer.h.20.ln_2.weight', 'dialog_encoder.transformer.h.20.ln_2.bias', 'dialog_encoder.transformer.h.20.mlp.c_fc.weight', 'dialog_encoder.transformer.h.20.mlp.c_fc.bias', 'dialog_encoder.transformer.h.20.mlp.c_proj.weight', 'dialog_encoder.transformer.h.20.mlp.c_proj.bias', 'dialog_encoder.transformer.h.21.ln_1.weight', 'dialog_encoder.transformer.h.21.ln_1.bias', 'dialog_encoder.transformer.h.21.attn.c_attn.weight', 'dialog_encoder.transformer.h.21.attn.c_attn.bias', 'dialog_encoder.transformer.h.21.attn.c_proj.weight', 'dialog_encoder.transformer.h.21.attn.c_proj.bias', 'dialog_encoder.transformer.h.21.ln_2.weight', 'dialog_encoder.transformer.h.21.ln_2.bias', 'dialog_encoder.transformer.h.21.mlp.c_fc.weight', 'dialog_encoder.transformer.h.21.mlp.c_fc.bias', 'dialog_encoder.transformer.h.21.mlp.c_proj.weight', 'dialog_encoder.transformer.h.21.mlp.c_proj.bias', 'dialog_encoder.transformer.h.22.ln_1.weight', 'dialog_encoder.transformer.h.22.ln_1.bias', 'dialog_encoder.transformer.h.22.attn.c_attn.weight', 'dialog_encoder.transformer.h.22.attn.c_attn.bias', 'dialog_encoder.transformer.h.22.attn.c_proj.weight', 'dialog_encoder.transformer.h.22.attn.c_proj.bias', 'dialog_encoder.transformer.h.22.ln_2.weight', 'dialog_encoder.transformer.h.22.ln_2.bias', 'dialog_encoder.transformer.h.22.mlp.c_fc.weight', 'dialog_encoder.transformer.h.22.mlp.c_fc.bias', 'dialog_encoder.transformer.h.22.mlp.c_proj.weight', 'dialog_encoder.transformer.h.22.mlp.c_proj.bias', 'dialog_encoder.transformer.h.23.ln_1.weight', 'dialog_encoder.transformer.h.23.ln_1.bias', 'dialog_encoder.transformer.h.23.attn.c_attn.weight', 'dialog_encoder.transformer.h.23.attn.c_attn.bias', 'dialog_encoder.transformer.h.23.attn.c_proj.weight', 'dialog_encoder.transformer.h.23.attn.c_proj.bias', 'dialog_encoder.transformer.h.23.ln_2.weight', 'dialog_encoder.transformer.h.23.ln_2.bias', 'dialog_encoder.transformer.h.23.mlp.c_fc.weight', 'dialog_encoder.transformer.h.23.mlp.c_fc.bias', 'dialog_encoder.transformer.h.23.mlp.c_proj.weight', 'dialog_encoder.transformer.h.23.mlp.c_proj.bias', 'dialog_encoder.transformer.ln_f.weight', 'dialog_encoder.transformer.ln_f.bias', 'dialog_encoder.lm_head.weight', 'question_encoder.transformer.wte.weight', 'question_encoder.transformer.wpe.weight', 'question_encoder.transformer.h.0.ln_1.weight', 'question_encoder.transformer.h.0.ln_1.bias', 'question_encoder.transformer.h.0.attn.c_attn.weight', 'question_encoder.transformer.h.0.attn.c_attn.bias', 'question_encoder.transformer.h.0.attn.c_proj.weight', 'question_encoder.transformer.h.0.attn.c_proj.bias', 'question_encoder.transformer.h.0.ln_2.weight', 'question_encoder.transformer.h.0.ln_2.bias', 'question_encoder.transformer.h.0.mlp.c_fc.weight', 'question_encoder.transformer.h.0.mlp.c_fc.bias', 'question_encoder.transformer.h.0.mlp.c_proj.weight', 'question_encoder.transformer.h.0.mlp.c_proj.bias', 'question_encoder.transformer.h.1.ln_1.weight', 'question_encoder.transformer.h.1.ln_1.bias', 'question_encoder.transformer.h.1.attn.c_attn.weight', 'question_encoder.transformer.h.1.attn.c_attn.bias', 'question_encoder.transformer.h.1.attn.c_proj.weight', 'question_encoder.transformer.h.1.attn.c_proj.bias', 'question_encoder.transformer.h.1.ln_2.weight', 'question_encoder.transformer.h.1.ln_2.bias', 'question_encoder.transformer.h.1.mlp.c_fc.weight', 'question_encoder.transformer.h.1.mlp.c_fc.bias', 'question_encoder.transformer.h.1.mlp.c_proj.weight', 'question_encoder.transformer.h.1.mlp.c_proj.bias', 'question_encoder.transformer.h.2.ln_1.weight', 'question_encoder.transformer.h.2.ln_1.bias', 'question_encoder.transformer.h.2.attn.c_attn.weight', 'question_encoder.transformer.h.2.attn.c_attn.bias', 'question_encoder.transformer.h.2.attn.c_proj.weight', 'question_encoder.transformer.h.2.attn.c_proj.bias', 'question_encoder.transformer.h.2.ln_2.weight', 'question_encoder.transformer.h.2.ln_2.bias', 'question_encoder.transformer.h.2.mlp.c_fc.weight', 'question_encoder.transformer.h.2.mlp.c_fc.bias', 'question_encoder.transformer.h.2.mlp.c_proj.weight', 'question_encoder.transformer.h.2.mlp.c_proj.bias', 'question_encoder.transformer.h.3.ln_1.weight', 'question_encoder.transformer.h.3.ln_1.bias', 'question_encoder.transformer.h.3.attn.c_attn.weight', 'question_encoder.transformer.h.3.attn.c_attn.bias', 'question_encoder.transformer.h.3.attn.c_proj.weight', 'question_encoder.transformer.h.3.attn.c_proj.bias', 'question_encoder.transformer.h.3.ln_2.weight', 'question_encoder.transformer.h.3.ln_2.bias', 'question_encoder.transformer.h.3.mlp.c_fc.weight', 'question_encoder.transformer.h.3.mlp.c_fc.bias', 'question_encoder.transformer.h.3.mlp.c_proj.weight', 'question_encoder.transformer.h.3.mlp.c_proj.bias', 'question_encoder.transformer.h.4.ln_1.weight', 'question_encoder.transformer.h.4.ln_1.bias', 'question_encoder.transformer.h.4.attn.c_attn.weight', 'question_encoder.transformer.h.4.attn.c_attn.bias', 'question_encoder.transformer.h.4.attn.c_proj.weight', 'question_encoder.transformer.h.4.attn.c_proj.bias', 'question_encoder.transformer.h.4.ln_2.weight', 'question_encoder.transformer.h.4.ln_2.bias', 'question_encoder.transformer.h.4.mlp.c_fc.weight', 'question_encoder.transformer.h.4.mlp.c_fc.bias', 'question_encoder.transformer.h.4.mlp.c_proj.weight', 'question_encoder.transformer.h.4.mlp.c_proj.bias', 'question_encoder.transformer.h.5.ln_1.weight', 'question_encoder.transformer.h.5.ln_1.bias', 'question_encoder.transformer.h.5.attn.c_attn.weight', 'question_encoder.transformer.h.5.attn.c_attn.bias', 'question_encoder.transformer.h.5.attn.c_proj.weight', 'question_encoder.transformer.h.5.attn.c_proj.bias', 'question_encoder.transformer.h.5.ln_2.weight', 'question_encoder.transformer.h.5.ln_2.bias', 'question_encoder.transformer.h.5.mlp.c_fc.weight', 'question_encoder.transformer.h.5.mlp.c_fc.bias', 'question_encoder.transformer.h.5.mlp.c_proj.weight', 'question_encoder.transformer.h.5.mlp.c_proj.bias', 'question_encoder.transformer.h.6.ln_1.weight', 'question_encoder.transformer.h.6.ln_1.bias', 'question_encoder.transformer.h.6.attn.c_attn.weight', 'question_encoder.transformer.h.6.attn.c_attn.bias', 'question_encoder.transformer.h.6.attn.c_proj.weight', 'question_encoder.transformer.h.6.attn.c_proj.bias', 'question_encoder.transformer.h.6.ln_2.weight', 'question_encoder.transformer.h.6.ln_2.bias', 'question_encoder.transformer.h.6.mlp.c_fc.weight', 'question_encoder.transformer.h.6.mlp.c_fc.bias', 'question_encoder.transformer.h.6.mlp.c_proj.weight', 'question_encoder.transformer.h.6.mlp.c_proj.bias', 'question_encoder.transformer.h.7.ln_1.weight', 'question_encoder.transformer.h.7.ln_1.bias', 'question_encoder.transformer.h.7.attn.c_attn.weight', 'question_encoder.transformer.h.7.attn.c_attn.bias', 'question_encoder.transformer.h.7.attn.c_proj.weight', 'question_encoder.transformer.h.7.attn.c_proj.bias', 'question_encoder.transformer.h.7.ln_2.weight', 'question_encoder.transformer.h.7.ln_2.bias', 'question_encoder.transformer.h.7.mlp.c_fc.weight', 'question_encoder.transformer.h.7.mlp.c_fc.bias', 'question_encoder.transformer.h.7.mlp.c_proj.weight', 'question_encoder.transformer.h.7.mlp.c_proj.bias', 'question_encoder.transformer.h.8.ln_1.weight', 'question_encoder.transformer.h.8.ln_1.bias', 'question_encoder.transformer.h.8.attn.c_attn.weight', 'question_encoder.transformer.h.8.attn.c_attn.bias', 'question_encoder.transformer.h.8.attn.c_proj.weight', 'question_encoder.transformer.h.8.attn.c_proj.bias', 'question_encoder.transformer.h.8.ln_2.weight', 'question_encoder.transformer.h.8.ln_2.bias', 'question_encoder.transformer.h.8.mlp.c_fc.weight', 'question_encoder.transformer.h.8.mlp.c_fc.bias', 'question_encoder.transformer.h.8.mlp.c_proj.weight', 'question_encoder.transformer.h.8.mlp.c_proj.bias', 'question_encoder.transformer.h.9.ln_1.weight', 'question_encoder.transformer.h.9.ln_1.bias', 'question_encoder.transformer.h.9.attn.c_attn.weight', 'question_encoder.transformer.h.9.attn.c_attn.bias', 'question_encoder.transformer.h.9.attn.c_proj.weight', 'question_encoder.transformer.h.9.attn.c_proj.bias', 'question_encoder.transformer.h.9.ln_2.weight', 'question_encoder.transformer.h.9.ln_2.bias', 'question_encoder.transformer.h.9.mlp.c_fc.weight', 'question_encoder.transformer.h.9.mlp.c_fc.bias', 'question_encoder.transformer.h.9.mlp.c_proj.weight', 'question_encoder.transformer.h.9.mlp.c_proj.bias', 'question_encoder.transformer.h.10.ln_1.weight', 'question_encoder.transformer.h.10.ln_1.bias', 'question_encoder.transformer.h.10.attn.c_attn.weight', 'question_encoder.transformer.h.10.attn.c_attn.bias', 'question_encoder.transformer.h.10.attn.c_proj.weight', 'question_encoder.transformer.h.10.attn.c_proj.bias', 'question_encoder.transformer.h.10.ln_2.weight', 'question_encoder.transformer.h.10.ln_2.bias', 'question_encoder.transformer.h.10.mlp.c_fc.weight', 'question_encoder.transformer.h.10.mlp.c_fc.bias', 'question_encoder.transformer.h.10.mlp.c_proj.weight', 'question_encoder.transformer.h.10.mlp.c_proj.bias', 'question_encoder.transformer.h.11.ln_1.weight', 'question_encoder.transformer.h.11.ln_1.bias', 'question_encoder.transformer.h.11.attn.c_attn.weight', 'question_encoder.transformer.h.11.attn.c_attn.bias', 'question_encoder.transformer.h.11.attn.c_proj.weight', 'question_encoder.transformer.h.11.attn.c_proj.bias', 'question_encoder.transformer.h.11.ln_2.weight', 'question_encoder.transformer.h.11.ln_2.bias', 'question_encoder.transformer.h.11.mlp.c_fc.weight', 'question_encoder.transformer.h.11.mlp.c_fc.bias', 'question_encoder.transformer.h.11.mlp.c_proj.weight', 'question_encoder.transformer.h.11.mlp.c_proj.bias', 'question_encoder.transformer.h.12.ln_1.weight', 'question_encoder.transformer.h.12.ln_1.bias', 'question_encoder.transformer.h.12.attn.c_attn.weight', 'question_encoder.transformer.h.12.attn.c_attn.bias', 'question_encoder.transformer.h.12.attn.c_proj.weight', 'question_encoder.transformer.h.12.attn.c_proj.bias', 'question_encoder.transformer.h.12.ln_2.weight', 'question_encoder.transformer.h.12.ln_2.bias', 'question_encoder.transformer.h.12.mlp.c_fc.weight', 'question_encoder.transformer.h.12.mlp.c_fc.bias', 'question_encoder.transformer.h.12.mlp.c_proj.weight', 'question_encoder.transformer.h.12.mlp.c_proj.bias', 'question_encoder.transformer.h.13.ln_1.weight', 'question_encoder.transformer.h.13.ln_1.bias', 'question_encoder.transformer.h.13.attn.c_attn.weight', 'question_encoder.transformer.h.13.attn.c_attn.bias', 'question_encoder.transformer.h.13.attn.c_proj.weight', 'question_encoder.transformer.h.13.attn.c_proj.bias', 'question_encoder.transformer.h.13.ln_2.weight', 'question_encoder.transformer.h.13.ln_2.bias', 'question_encoder.transformer.h.13.mlp.c_fc.weight', 'question_encoder.transformer.h.13.mlp.c_fc.bias', 'question_encoder.transformer.h.13.mlp.c_proj.weight', 'question_encoder.transformer.h.13.mlp.c_proj.bias', 'question_encoder.transformer.h.14.ln_1.weight', 'question_encoder.transformer.h.14.ln_1.bias', 'question_encoder.transformer.h.14.attn.c_attn.weight', 'question_encoder.transformer.h.14.attn.c_attn.bias', 'question_encoder.transformer.h.14.attn.c_proj.weight', 'question_encoder.transformer.h.14.attn.c_proj.bias', 'question_encoder.transformer.h.14.ln_2.weight', 'question_encoder.transformer.h.14.ln_2.bias', 'question_encoder.transformer.h.14.mlp.c_fc.weight', 'question_encoder.transformer.h.14.mlp.c_fc.bias', 'question_encoder.transformer.h.14.mlp.c_proj.weight', 'question_encoder.transformer.h.14.mlp.c_proj.bias', 'question_encoder.transformer.h.15.ln_1.weight', 'question_encoder.transformer.h.15.ln_1.bias', 'question_encoder.transformer.h.15.attn.c_attn.weight', 'question_encoder.transformer.h.15.attn.c_attn.bias', 'question_encoder.transformer.h.15.attn.c_proj.weight', 'question_encoder.transformer.h.15.attn.c_proj.bias', 'question_encoder.transformer.h.15.ln_2.weight', 'question_encoder.transformer.h.15.ln_2.bias', 'question_encoder.transformer.h.15.mlp.c_fc.weight', 'question_encoder.transformer.h.15.mlp.c_fc.bias', 'question_encoder.transformer.h.15.mlp.c_proj.weight', 'question_encoder.transformer.h.15.mlp.c_proj.bias', 'question_encoder.transformer.h.16.ln_1.weight', 'question_encoder.transformer.h.16.ln_1.bias', 'question_encoder.transformer.h.16.attn.c_attn.weight', 'question_encoder.transformer.h.16.attn.c_attn.bias', 'question_encoder.transformer.h.16.attn.c_proj.weight', 'question_encoder.transformer.h.16.attn.c_proj.bias', 'question_encoder.transformer.h.16.ln_2.weight', 'question_encoder.transformer.h.16.ln_2.bias', 'question_encoder.transformer.h.16.mlp.c_fc.weight', 'question_encoder.transformer.h.16.mlp.c_fc.bias', 'question_encoder.transformer.h.16.mlp.c_proj.weight', 'question_encoder.transformer.h.16.mlp.c_proj.bias', 'question_encoder.transformer.h.17.ln_1.weight', 'question_encoder.transformer.h.17.ln_1.bias', 'question_encoder.transformer.h.17.attn.c_attn.weight', 'question_encoder.transformer.h.17.attn.c_attn.bias', 'question_encoder.transformer.h.17.attn.c_proj.weight', 'question_encoder.transformer.h.17.attn.c_proj.bias', 'question_encoder.transformer.h.17.ln_2.weight', 'question_encoder.transformer.h.17.ln_2.bias', 'question_encoder.transformer.h.17.mlp.c_fc.weight', 'question_encoder.transformer.h.17.mlp.c_fc.bias', 'question_encoder.transformer.h.17.mlp.c_proj.weight', 'question_encoder.transformer.h.17.mlp.c_proj.bias', 'question_encoder.transformer.h.18.ln_1.weight', 'question_encoder.transformer.h.18.ln_1.bias', 'question_encoder.transformer.h.18.attn.c_attn.weight', 'question_encoder.transformer.h.18.attn.c_attn.bias', 'question_encoder.transformer.h.18.attn.c_proj.weight', 'question_encoder.transformer.h.18.attn.c_proj.bias', 'question_encoder.transformer.h.18.ln_2.weight', 'question_encoder.transformer.h.18.ln_2.bias', 'question_encoder.transformer.h.18.mlp.c_fc.weight', 'question_encoder.transformer.h.18.mlp.c_fc.bias', 'question_encoder.transformer.h.18.mlp.c_proj.weight', 'question_encoder.transformer.h.18.mlp.c_proj.bias', 'question_encoder.transformer.h.19.ln_1.weight', 'question_encoder.transformer.h.19.ln_1.bias', 'question_encoder.transformer.h.19.attn.c_attn.weight', 'question_encoder.transformer.h.19.attn.c_attn.bias', 'question_encoder.transformer.h.19.attn.c_proj.weight', 'question_encoder.transformer.h.19.attn.c_proj.bias', 'question_encoder.transformer.h.19.ln_2.weight', 'question_encoder.transformer.h.19.ln_2.bias', 'question_encoder.transformer.h.19.mlp.c_fc.weight', 'question_encoder.transformer.h.19.mlp.c_fc.bias', 'question_encoder.transformer.h.19.mlp.c_proj.weight', 'question_encoder.transformer.h.19.mlp.c_proj.bias', 'question_encoder.transformer.h.20.ln_1.weight', 'question_encoder.transformer.h.20.ln_1.bias', 'question_encoder.transformer.h.20.attn.c_attn.weight', 'question_encoder.transformer.h.20.attn.c_attn.bias', 'question_encoder.transformer.h.20.attn.c_proj.weight', 'question_encoder.transformer.h.20.attn.c_proj.bias', 'question_encoder.transformer.h.20.ln_2.weight', 'question_encoder.transformer.h.20.ln_2.bias', 'question_encoder.transformer.h.20.mlp.c_fc.weight', 'question_encoder.transformer.h.20.mlp.c_fc.bias', 'question_encoder.transformer.h.20.mlp.c_proj.weight', 'question_encoder.transformer.h.20.mlp.c_proj.bias', 'question_encoder.transformer.h.21.ln_1.weight', 'question_encoder.transformer.h.21.ln_1.bias', 'question_encoder.transformer.h.21.attn.c_attn.weight', 'question_encoder.transformer.h.21.attn.c_attn.bias', 'question_encoder.transformer.h.21.attn.c_proj.weight', 'question_encoder.transformer.h.21.attn.c_proj.bias', 'question_encoder.transformer.h.21.ln_2.weight', 'question_encoder.transformer.h.21.ln_2.bias', 'question_encoder.transformer.h.21.mlp.c_fc.weight', 'question_encoder.transformer.h.21.mlp.c_fc.bias', 'question_encoder.transformer.h.21.mlp.c_proj.weight', 'question_encoder.transformer.h.21.mlp.c_proj.bias', 'question_encoder.transformer.h.22.ln_1.weight', 'question_encoder.transformer.h.22.ln_1.bias', 'question_encoder.transformer.h.22.attn.c_attn.weight', 'question_encoder.transformer.h.22.attn.c_attn.bias', 'question_encoder.transformer.h.22.attn.c_proj.weight', 'question_encoder.transformer.h.22.attn.c_proj.bias', 'question_encoder.transformer.h.22.ln_2.weight', 'question_encoder.transformer.h.22.ln_2.bias', 'question_encoder.transformer.h.22.mlp.c_fc.weight', 'question_encoder.transformer.h.22.mlp.c_fc.bias', 'question_encoder.transformer.h.22.mlp.c_proj.weight', 'question_encoder.transformer.h.22.mlp.c_proj.bias', 'question_encoder.transformer.h.23.ln_1.weight', 'question_encoder.transformer.h.23.ln_1.bias', 'question_encoder.transformer.h.23.attn.c_attn.weight', 'question_encoder.transformer.h.23.attn.c_attn.bias', 'question_encoder.transformer.h.23.attn.c_proj.weight', 'question_encoder.transformer.h.23.attn.c_proj.bias', 'question_encoder.transformer.h.23.ln_2.weight', 'question_encoder.transformer.h.23.ln_2.bias', 'question_encoder.transformer.h.23.mlp.c_fc.weight', 'question_encoder.transformer.h.23.mlp.c_fc.bias', 'question_encoder.transformer.h.23.mlp.c_proj.weight', 'question_encoder.transformer.h.23.mlp.c_proj.bias', 'question_encoder.transformer.ln_f.weight', 'question_encoder.transformer.ln_f.bias', 'question_encoder.lm_head.weight', 'decoder.transformer.wte.weight', 'decoder.transformer.wpe.weight', 'decoder.transformer.h.0.ln_1.weight', 'decoder.transformer.h.0.ln_1.bias', 'decoder.transformer.h.0.attn.c_attn.weight', 'decoder.transformer.h.0.attn.c_attn.bias', 'decoder.transformer.h.0.attn.c_proj.weight', 'decoder.transformer.h.0.attn.c_proj.bias', 'decoder.transformer.h.0.ln_2.weight', 'decoder.transformer.h.0.ln_2.bias', 'decoder.transformer.h.0.mlp.c_fc.weight', 'decoder.transformer.h.0.mlp.c_fc.bias', 'decoder.transformer.h.0.mlp.c_proj.weight', 'decoder.transformer.h.0.mlp.c_proj.bias', 'decoder.transformer.h.1.ln_1.weight', 'decoder.transformer.h.1.ln_1.bias', 'decoder.transformer.h.1.attn.c_attn.weight', 'decoder.transformer.h.1.attn.c_attn.bias', 'decoder.transformer.h.1.attn.c_proj.weight', 'decoder.transformer.h.1.attn.c_proj.bias', 'decoder.transformer.h.1.ln_2.weight', 'decoder.transformer.h.1.ln_2.bias', 'decoder.transformer.h.1.mlp.c_fc.weight', 'decoder.transformer.h.1.mlp.c_fc.bias', 'decoder.transformer.h.1.mlp.c_proj.weight', 'decoder.transformer.h.1.mlp.c_proj.bias', 'decoder.transformer.h.2.ln_1.weight', 'decoder.transformer.h.2.ln_1.bias', 'decoder.transformer.h.2.attn.c_attn.weight', 'decoder.transformer.h.2.attn.c_attn.bias', 'decoder.transformer.h.2.attn.c_proj.weight', 'decoder.transformer.h.2.attn.c_proj.bias', 'decoder.transformer.h.2.ln_2.weight', 'decoder.transformer.h.2.ln_2.bias', 'decoder.transformer.h.2.mlp.c_fc.weight', 'decoder.transformer.h.2.mlp.c_fc.bias', 'decoder.transformer.h.2.mlp.c_proj.weight', 'decoder.transformer.h.2.mlp.c_proj.bias', 'decoder.transformer.h.3.ln_1.weight', 'decoder.transformer.h.3.ln_1.bias', 'decoder.transformer.h.3.attn.c_attn.weight', 'decoder.transformer.h.3.attn.c_attn.bias', 'decoder.transformer.h.3.attn.c_proj.weight', 'decoder.transformer.h.3.attn.c_proj.bias', 'decoder.transformer.h.3.ln_2.weight', 'decoder.transformer.h.3.ln_2.bias', 'decoder.transformer.h.3.mlp.c_fc.weight', 'decoder.transformer.h.3.mlp.c_fc.bias', 'decoder.transformer.h.3.mlp.c_proj.weight', 'decoder.transformer.h.3.mlp.c_proj.bias', 'decoder.transformer.h.4.ln_1.weight', 'decoder.transformer.h.4.ln_1.bias', 'decoder.transformer.h.4.attn.c_attn.weight', 'decoder.transformer.h.4.attn.c_attn.bias', 'decoder.transformer.h.4.attn.c_proj.weight', 'decoder.transformer.h.4.attn.c_proj.bias', 'decoder.transformer.h.4.ln_2.weight', 'decoder.transformer.h.4.ln_2.bias', 'decoder.transformer.h.4.mlp.c_fc.weight', 'decoder.transformer.h.4.mlp.c_fc.bias', 'decoder.transformer.h.4.mlp.c_proj.weight', 'decoder.transformer.h.4.mlp.c_proj.bias', 'decoder.transformer.h.5.ln_1.weight', 'decoder.transformer.h.5.ln_1.bias', 'decoder.transformer.h.5.attn.c_attn.weight', 'decoder.transformer.h.5.attn.c_attn.bias', 'decoder.transformer.h.5.attn.c_proj.weight', 'decoder.transformer.h.5.attn.c_proj.bias', 'decoder.transformer.h.5.ln_2.weight', 'decoder.transformer.h.5.ln_2.bias', 'decoder.transformer.h.5.mlp.c_fc.weight', 'decoder.transformer.h.5.mlp.c_fc.bias', 'decoder.transformer.h.5.mlp.c_proj.weight', 'decoder.transformer.h.5.mlp.c_proj.bias', 'decoder.transformer.h.6.ln_1.weight', 'decoder.transformer.h.6.ln_1.bias', 'decoder.transformer.h.6.attn.c_attn.weight', 'decoder.transformer.h.6.attn.c_attn.bias', 'decoder.transformer.h.6.attn.c_proj.weight', 'decoder.transformer.h.6.attn.c_proj.bias', 'decoder.transformer.h.6.ln_2.weight', 'decoder.transformer.h.6.ln_2.bias', 'decoder.transformer.h.6.mlp.c_fc.weight', 'decoder.transformer.h.6.mlp.c_fc.bias', 'decoder.transformer.h.6.mlp.c_proj.weight', 'decoder.transformer.h.6.mlp.c_proj.bias', 'decoder.transformer.h.7.ln_1.weight', 'decoder.transformer.h.7.ln_1.bias', 'decoder.transformer.h.7.attn.c_attn.weight', 'decoder.transformer.h.7.attn.c_attn.bias', 'decoder.transformer.h.7.attn.c_proj.weight', 'decoder.transformer.h.7.attn.c_proj.bias', 'decoder.transformer.h.7.ln_2.weight', 'decoder.transformer.h.7.ln_2.bias', 'decoder.transformer.h.7.mlp.c_fc.weight', 'decoder.transformer.h.7.mlp.c_fc.bias', 'decoder.transformer.h.7.mlp.c_proj.weight', 'decoder.transformer.h.7.mlp.c_proj.bias', 'decoder.transformer.h.8.ln_1.weight', 'decoder.transformer.h.8.ln_1.bias', 'decoder.transformer.h.8.attn.c_attn.weight', 'decoder.transformer.h.8.attn.c_attn.bias', 'decoder.transformer.h.8.attn.c_proj.weight', 'decoder.transformer.h.8.attn.c_proj.bias', 'decoder.transformer.h.8.ln_2.weight', 'decoder.transformer.h.8.ln_2.bias', 'decoder.transformer.h.8.mlp.c_fc.weight', 'decoder.transformer.h.8.mlp.c_fc.bias', 'decoder.transformer.h.8.mlp.c_proj.weight', 'decoder.transformer.h.8.mlp.c_proj.bias', 'decoder.transformer.h.9.ln_1.weight', 'decoder.transformer.h.9.ln_1.bias', 'decoder.transformer.h.9.attn.c_attn.weight', 'decoder.transformer.h.9.attn.c_attn.bias', 'decoder.transformer.h.9.attn.c_proj.weight', 'decoder.transformer.h.9.attn.c_proj.bias', 'decoder.transformer.h.9.ln_2.weight', 'decoder.transformer.h.9.ln_2.bias', 'decoder.transformer.h.9.mlp.c_fc.weight', 'decoder.transformer.h.9.mlp.c_fc.bias', 'decoder.transformer.h.9.mlp.c_proj.weight', 'decoder.transformer.h.9.mlp.c_proj.bias', 'decoder.transformer.h.10.ln_1.weight', 'decoder.transformer.h.10.ln_1.bias', 'decoder.transformer.h.10.attn.c_attn.weight', 'decoder.transformer.h.10.attn.c_attn.bias', 'decoder.transformer.h.10.attn.c_proj.weight', 'decoder.transformer.h.10.attn.c_proj.bias', 'decoder.transformer.h.10.ln_2.weight', 'decoder.transformer.h.10.ln_2.bias', 'decoder.transformer.h.10.mlp.c_fc.weight', 'decoder.transformer.h.10.mlp.c_fc.bias', 'decoder.transformer.h.10.mlp.c_proj.weight', 'decoder.transformer.h.10.mlp.c_proj.bias', 'decoder.transformer.h.11.ln_1.weight', 'decoder.transformer.h.11.ln_1.bias', 'decoder.transformer.h.11.attn.c_attn.weight', 'decoder.transformer.h.11.attn.c_attn.bias', 'decoder.transformer.h.11.attn.c_proj.weight', 'decoder.transformer.h.11.attn.c_proj.bias', 'decoder.transformer.h.11.ln_2.weight', 'decoder.transformer.h.11.ln_2.bias', 'decoder.transformer.h.11.mlp.c_fc.weight', 'decoder.transformer.h.11.mlp.c_fc.bias', 'decoder.transformer.h.11.mlp.c_proj.weight', 'decoder.transformer.h.11.mlp.c_proj.bias', 'decoder.transformer.h.12.ln_1.weight', 'decoder.transformer.h.12.ln_1.bias', 'decoder.transformer.h.12.attn.c_attn.weight', 'decoder.transformer.h.12.attn.c_attn.bias', 'decoder.transformer.h.12.attn.c_proj.weight', 'decoder.transformer.h.12.attn.c_proj.bias', 'decoder.transformer.h.12.ln_2.weight', 'decoder.transformer.h.12.ln_2.bias', 'decoder.transformer.h.12.mlp.c_fc.weight', 'decoder.transformer.h.12.mlp.c_fc.bias', 'decoder.transformer.h.12.mlp.c_proj.weight', 'decoder.transformer.h.12.mlp.c_proj.bias', 'decoder.transformer.h.13.ln_1.weight', 'decoder.transformer.h.13.ln_1.bias', 'decoder.transformer.h.13.attn.c_attn.weight', 'decoder.transformer.h.13.attn.c_attn.bias', 'decoder.transformer.h.13.attn.c_proj.weight', 'decoder.transformer.h.13.attn.c_proj.bias', 'decoder.transformer.h.13.ln_2.weight', 'decoder.transformer.h.13.ln_2.bias', 'decoder.transformer.h.13.mlp.c_fc.weight', 'decoder.transformer.h.13.mlp.c_fc.bias', 'decoder.transformer.h.13.mlp.c_proj.weight', 'decoder.transformer.h.13.mlp.c_proj.bias', 'decoder.transformer.h.14.ln_1.weight', 'decoder.transformer.h.14.ln_1.bias', 'decoder.transformer.h.14.attn.c_attn.weight', 'decoder.transformer.h.14.attn.c_attn.bias', 'decoder.transformer.h.14.attn.c_proj.weight', 'decoder.transformer.h.14.attn.c_proj.bias', 'decoder.transformer.h.14.ln_2.weight', 'decoder.transformer.h.14.ln_2.bias', 'decoder.transformer.h.14.mlp.c_fc.weight', 'decoder.transformer.h.14.mlp.c_fc.bias', 'decoder.transformer.h.14.mlp.c_proj.weight', 'decoder.transformer.h.14.mlp.c_proj.bias', 'decoder.transformer.h.15.ln_1.weight', 'decoder.transformer.h.15.ln_1.bias', 'decoder.transformer.h.15.attn.c_attn.weight', 'decoder.transformer.h.15.attn.c_attn.bias', 'decoder.transformer.h.15.attn.c_proj.weight', 'decoder.transformer.h.15.attn.c_proj.bias', 'decoder.transformer.h.15.ln_2.weight', 'decoder.transformer.h.15.ln_2.bias', 'decoder.transformer.h.15.mlp.c_fc.weight', 'decoder.transformer.h.15.mlp.c_fc.bias', 'decoder.transformer.h.15.mlp.c_proj.weight', 'decoder.transformer.h.15.mlp.c_proj.bias', 'decoder.transformer.h.16.ln_1.weight', 'decoder.transformer.h.16.ln_1.bias', 'decoder.transformer.h.16.attn.c_attn.weight', 'decoder.transformer.h.16.attn.c_attn.bias', 'decoder.transformer.h.16.attn.c_proj.weight', 'decoder.transformer.h.16.attn.c_proj.bias', 'decoder.transformer.h.16.ln_2.weight', 'decoder.transformer.h.16.ln_2.bias', 'decoder.transformer.h.16.mlp.c_fc.weight', 'decoder.transformer.h.16.mlp.c_fc.bias', 'decoder.transformer.h.16.mlp.c_proj.weight', 'decoder.transformer.h.16.mlp.c_proj.bias', 'decoder.transformer.h.17.ln_1.weight', 'decoder.transformer.h.17.ln_1.bias', 'decoder.transformer.h.17.attn.c_attn.weight', 'decoder.transformer.h.17.attn.c_attn.bias', 'decoder.transformer.h.17.attn.c_proj.weight', 'decoder.transformer.h.17.attn.c_proj.bias', 'decoder.transformer.h.17.ln_2.weight', 'decoder.transformer.h.17.ln_2.bias', 'decoder.transformer.h.17.mlp.c_fc.weight', 'decoder.transformer.h.17.mlp.c_fc.bias', 'decoder.transformer.h.17.mlp.c_proj.weight', 'decoder.transformer.h.17.mlp.c_proj.bias', 'decoder.transformer.h.18.ln_1.weight', 'decoder.transformer.h.18.ln_1.bias', 'decoder.transformer.h.18.attn.c_attn.weight', 'decoder.transformer.h.18.attn.c_attn.bias', 'decoder.transformer.h.18.attn.c_proj.weight', 'decoder.transformer.h.18.attn.c_proj.bias', 'decoder.transformer.h.18.ln_2.weight', 'decoder.transformer.h.18.ln_2.bias', 'decoder.transformer.h.18.mlp.c_fc.weight', 'decoder.transformer.h.18.mlp.c_fc.bias', 'decoder.transformer.h.18.mlp.c_proj.weight', 'decoder.transformer.h.18.mlp.c_proj.bias', 'decoder.transformer.h.19.ln_1.weight', 'decoder.transformer.h.19.ln_1.bias', 'decoder.transformer.h.19.attn.c_attn.weight', 'decoder.transformer.h.19.attn.c_attn.bias', 'decoder.transformer.h.19.attn.c_proj.weight', 'decoder.transformer.h.19.attn.c_proj.bias', 'decoder.transformer.h.19.ln_2.weight', 'decoder.transformer.h.19.ln_2.bias', 'decoder.transformer.h.19.mlp.c_fc.weight', 'decoder.transformer.h.19.mlp.c_fc.bias', 'decoder.transformer.h.19.mlp.c_proj.weight', 'decoder.transformer.h.19.mlp.c_proj.bias', 'decoder.transformer.h.20.ln_1.weight', 'decoder.transformer.h.20.ln_1.bias', 'decoder.transformer.h.20.attn.c_attn.weight', 'decoder.transformer.h.20.attn.c_attn.bias', 'decoder.transformer.h.20.attn.c_proj.weight', 'decoder.transformer.h.20.attn.c_proj.bias', 'decoder.transformer.h.20.ln_2.weight', 'decoder.transformer.h.20.ln_2.bias', 'decoder.transformer.h.20.mlp.c_fc.weight', 'decoder.transformer.h.20.mlp.c_fc.bias', 'decoder.transformer.h.20.mlp.c_proj.weight', 'decoder.transformer.h.20.mlp.c_proj.bias', 'decoder.transformer.h.21.ln_1.weight', 'decoder.transformer.h.21.ln_1.bias', 'decoder.transformer.h.21.attn.c_attn.weight', 'decoder.transformer.h.21.attn.c_attn.bias', 'decoder.transformer.h.21.attn.c_proj.weight', 'decoder.transformer.h.21.attn.c_proj.bias', 'decoder.transformer.h.21.ln_2.weight', 'decoder.transformer.h.21.ln_2.bias', 'decoder.transformer.h.21.mlp.c_fc.weight', 'decoder.transformer.h.21.mlp.c_fc.bias', 'decoder.transformer.h.21.mlp.c_proj.weight', 'decoder.transformer.h.21.mlp.c_proj.bias', 'decoder.transformer.h.22.ln_1.weight', 'decoder.transformer.h.22.ln_1.bias', 'decoder.transformer.h.22.attn.c_attn.weight', 'decoder.transformer.h.22.attn.c_attn.bias', 'decoder.transformer.h.22.attn.c_proj.weight', 'decoder.transformer.h.22.attn.c_proj.bias', 'decoder.transformer.h.22.ln_2.weight', 'decoder.transformer.h.22.ln_2.bias', 'decoder.transformer.h.22.mlp.c_fc.weight', 'decoder.transformer.h.22.mlp.c_fc.bias', 'decoder.transformer.h.22.mlp.c_proj.weight', 'decoder.transformer.h.22.mlp.c_proj.bias', 'decoder.transformer.h.23.ln_1.weight', 'decoder.transformer.h.23.ln_1.bias', 'decoder.transformer.h.23.attn.c_attn.weight', 'decoder.transformer.h.23.attn.c_attn.bias', 'decoder.transformer.h.23.attn.c_proj.weight', 'decoder.transformer.h.23.attn.c_proj.bias', 'decoder.transformer.h.23.ln_2.weight', 'decoder.transformer.h.23.ln_2.bias', 'decoder.transformer.h.23.mlp.c_fc.weight', 'decoder.transformer.h.23.mlp.c_fc.bias', 'decoder.transformer.h.23.mlp.c_proj.weight', 'decoder.transformer.h.23.mlp.c_proj.bias', 'decoder.transformer.ln_f.weight', 'decoder.transformer.ln_f.bias', 'decoder.lm_head.weight', 'fusion_layer.attention.weight', 'fusion_layer.attention.bias', 'fusion_layer.decoder.weight_ih_l0', 'fusion_layer.decoder.weight_hh_l0', 'fusion_layer.decoder.bias_ih_l0', 'fusion_layer.decoder.bias_hh_l0', 'fusion_layer.decoder.weight_ih_l1', 'fusion_layer.decoder.weight_hh_l1', 'fusion_layer.decoder.bias_ih_l1', 'fusion_layer.decoder.bias_hh_l1', 'fusion_layer.linear.weight', 'fusion_layer.linear.bias'])\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "metadata": {},
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data_loader = DataLoader(valDataset, batch_size=1, shuffle=True)\n",
        "\n",
        "# Extract the first batch\n",
        "first_batch = next(iter(data_loader))\n",
        "\n",
        "Qdialog_tokens, Adialog_tokens, Question_tokens, image, answer = first_batch"
      ],
      "metadata": {
        "id": "9jBAXocGYoaU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "49691d06-7b5b-4a6c-e536-28229717b493"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "10650059 Question:## We have a file repository containing code files `*.sql; *.xep; *.dll; *.aspx; *.gif` that you are going to submit to Production pretty soon. In this repository we have the main folders which contain the most recent code files to go to PROD, but we also have all the Change Folders that where submitted to PRE-PROD with code. You can see the main repository structure in the image below:\n",
            "\n",
            "![enter image description here](https://i.stack.imgur.com/Iode8.jpg)\n",
            "\n",
            "The protocol is that whenever we submit something to PRE-PROD, we e create a Change Folder, place it in the main repository, and also update the main folders, but sometimes we forget to do the second part.\n",
            "What i was trying to do in a automated way is: if there is a file with the same name in the main folder and the change folder they need the have the same modified date day (at least), this specific crossing i could easily do in Excel or even SQL.\n",
            "So, finally :P , what i needed help in, is getting into a csv fileformat (';' separating values, and '\\n' separating rows), all the `*.sql; *.xep; *.dll; *.aspx; *.gif` from the main repository directory and sub-directories.\n",
            "\n",
            "So far I i have tested with this dos commands:\n",
            "\n",
            "```\n",
            "dir *.sql *.xep *.aspx *.dll *.gif /s /a:-D>listWithDate.txt\n",
            "```\n",
            "\n",
            "\n",
            "this one gets me a list, ie: listWithDate.txt, that a i have formated in this fashion:\n",
            "\n",
            "```\n",
            "2012/03/19[2sapces]14:27[Nspaces]4.006[1space][filename]\n",
            "2012/03/19[2sapces]14:27[Nspaces]10.006[1space][filename]\n",
            "```\n",
            "\n",
            "\n",
            "So needed help in on of the two:\n",
            "\n",
            "\n",
            "\n",
            "or\n",
            "\n",
            "\n",
            "\n",
            "Many thanks in advance ;)\n",
            " Dialogs:#### [['343955 : Basically, you want to generate a CSV using the list, if I am right?', '799586 : @adarshr I was editing at the same as you, and mine had more editing so I think it took precedence over yours.', '343955 : @BaliC Not a problem, I have fixed now. By the way, it was David who was editing then. Not me.', '1384237 : tks for the editing fellows :)\\n@adarshr yes basically that, just need 2 cols, Date Modified, and File Name.']] Image path: https://i.stack.imgur.com/Iode8.jpg answer:##### The following one-liner will output file timestamps (as single units), file sizes (in bytes), and file names as a CSV row set:\n",
            "\n",
            "```\n",
            ">csvfile.txt (FOR %R IN (*.sql *.xep *.aspx *.dll *.gif) DO @ECHO \"%~tR\";%~zR;\"%~nxR\")\n",
            "```\n",
            "\n",
            "\n",
            "Note: this is supposed to be run directly from the command prompt. If you wish to put this in a batch script, double every `%` character.\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# image = image_processor(images=image, return_tensors=\"pt\").to(device)\n",
        "i = 0\n",
        "outputs = []\n",
        "for Qdialog_tokens, Adialog_tokens, Question_tokens, image, answer in data_loader:\n",
        "\n",
        "  print(f\"Datas {i}##################################################################\")\n",
        "  # image = image_processor(images=image, return_tensors=\"pt\").to(device)\n",
        "  with torch.no_grad():\n",
        "      image = image_processor(images=image, return_tensors=\"pt\").to(device)\n",
        "      output,_ = model(image, Question_tokens, Adialog_tokens, Question_tokens, answer)\n",
        "      print(\"######################################################################\")\n",
        "      i+=1\n",
        "      outputs.append(output)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "TCXJGAcTdA3t",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e386de7d-dac2-4a1e-a7de-7c18f84cfcbf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "14022257 Question:## Eclipse is closing while loading any XML layout with this message `Java was started but returned exit code -1073741571`\n",
            "\n",
            "\n",
            "\n",
            "```\n",
            "-startup\n",
            "plugins/org.eclipse.equinox.launcher_1.3.0.v20120522-1813.jar\n",
            "--launcher.library\n",
            "plugins/org.eclipse.equinox.launcher.win32.win32.x86_64_1.1.200.v20120522-1813\n",
            "-product\n",
            "com.android.ide.eclipse.adt.package.product\n",
            "--launcher.XXMaxPermSize\n",
            "256M\n",
            "-showsplash\n",
            "com.android.ide.eclipse.adt.package.product\n",
            "--launcher.XXMaxPermSize\n",
            "256m\n",
            "--launcher.defaultAction\n",
            "openFile\n",
            "-vmargs\n",
            "-Dosgi.requiredJavaVersion=1.6\n",
            "-Xms512m\n",
            "-Xmx1024m\n",
            "-Xss1024k\n",
            "-Declipse.buildId=v21.0.0-519525\n",
            "```\n",
            "\n",
            "\n",
            "![enter image description here](https://i.stack.imgur.com/KjkZ4.jpg)\n",
            " Dialogs:#### [['1360074 : There are at least 3 question with \"Java was started but returned exit code\" at SO. None of them help?', \"1029916 : @NikolayKuznetsov Unless the exit code was the same I don't see them being very helpful to the OP. This http://support.microsoft.com/kb/838501 might be useful.\", \"488434 : @NominSim: do you suspect issue in my windows? solution? it's seems something in the registry keys\"]] Image path: https://i.stack.imgur.com/KjkZ4.jpg answer:##### Thanks everyone for sharing solutions with me it seem that I had corrupted xml ( I am still not sure what corrupted  means but it was forcing the IDE to crash) here is a snippet:\n",
            "\n",
            "```\n",
            "<com.android.example.EndlesScrollView...\n",
            "\n",
            "    <FrameLayout...\n",
            "        <LinearLayout...\n",
            "```\n",
            "\n",
            "\n",
            "Each time I tried to use `Graphical layout` view in eclipse the IDE crashes, All I did was to delete that XML totally and re-create it from scratch, it seems something wrong with the XML Tags which preventing the `Graphical layout` from correctly displaying the correct UI.\n",
            "\n",
            "Datas 0##################################################################\n",
            "######################################################################\n",
            "12415277 Question:## I'm seeing other questions posted re iOS 6 so I hope its now kosher to ask them here ...\n",
            "\n",
            "I am testing a published app built using Xcode GM 4.5.  I am getting a crash when loading a table view controller on a device running GM iOS 6 firmware. The crash doesn't happen when building to devices running iOS 5.1 or 4.3.5.  I am infering (perhaps incorrectly) from the error included below that there must be a problem with the way the table view's outlet is connected but as it works in 5.1 and 4.3.5 and the code is identical as to what is happening in iOS 6 I am not clear as to what is going wrong.\n",
            "\n",
            "I've deleted previous versions of the app from the device, reset it and cleaned the project but that does not help.\n",
            "\n",
            "I've looked at release notes but am not seeing anything that points to what needs to be done in iOS 6.\n",
            "\n",
            "The error is:\n",
            "\n",
            "```\n",
            "*** Terminating app due to uncaught exception 'NSInternalInconsistencyException', reason: '-[UITableViewController loadView] loaded the \"XViewController\" nib but didn't get a UITableView.'\n",
            "```\n",
            "\n",
            "\n",
            "Thanks for any pointers on what I need to do to fix this.\n",
            "\n",
            "-- Additional info:\n",
            "\n",
            "(Note: XViewController is actually LogViewController in the app.)\n",
            "\n",
            "In the class interface file I have the outlet declared as:\n",
            "\n",
            "```\n",
            "@property (strong, nonatomic) IBOutlet UITableView *logTableView;\n",
            "```\n",
            "\n",
            "\n",
            "In the xib here is the connections inspector for File's Owner:\n",
            "\n",
            "![enter image description here](https://i.stack.imgur.com/6pQPH.png)\n",
            " Dialogs:#### [['803787 : The exception seems pretty clear and explicit to me', '472344 : @AliSoftware In that case you should post an answer.', '635746 : I agree @Darren. Perhaps my question is \"dumb\" or the solution is obvious, but isn\\'t the purpose of SO to assist others who are sincerely trying to figure things out as opposed to being down voted for asking questions they have researched and tried to solve on their own but are stuck on? This has not been my experience in the past on SO. I may have found a clue to an answer to this issue at http://stackoverflow.com/questions/11137669/xcode-4-5-corrupting-xibs, but the solutions therein are not working for me to solve this problem.']] Image path: https://i.stack.imgur.com/6pQPH.png answer:##### The exception `[UITableViewController loadView] loaded the \"XViewController\" nib but didn't get a UITableView.` means that your \"XViewController.xib\" file contains an `UITableViewController` class (probably its File's Owner?) whose `view` IBOutlet is binded to something other than an `UITableView`.\n",
            "\n",
            "`UITableViewController``view``UITableView` (or one of its custom subclass if you created any), and `UIView` (even if that `UIView` contain some `UITableView` in its subclass or whatever) like in your capture.\n",
            "\n",
            "Datas 1##################################################################\n",
            "######################################################################\n",
            "16964294 Question:## Is it possible to evenly space many elements in a div with changeable width.\n",
            "\n",
            "Here's [not working example](http://jsfiddle.net/ULQwf/293/). If we use text-align:center; elements will be centered, but margin:0 auto; is not working. I want to accomplish something like justify+center:\n",
            "\n",
            "```\n",
            "|..<elem>..<elem>..<elem>..<elem>..|       // for one container width\n",
            "|..<elem>..<elem>..<elem>..|               // for smaller container width\n",
            "|....<elem>....<elem>....|                 // even smaller container\n",
            "```\n",
            "\n",
            "\n",
            "Container will be user resizable. \n",
            "One picture is worth a 1000 words:  \n",
            "\n",
            "![enter image description here](https://i.stack.imgur.com/yDPQa.png)\n",
            "\n",
            "Container(red box) width:100%; So user can resize it (browser window, js, whatever).\n",
            "<--> represent even spaces. \n",
            "In second row <--> are bigger because there is more room. I was able to fake it with:\n",
            "\n",
            "```\n",
            "text-align:center;\n",
            "word-spacing:3em;    // but any fixed value looses proportion\n",
            "```\n",
            "\n",
            " Dialogs:#### [[\"352260 : @ rink.attendant.6: yes, that picture is exactly what I want. elements are the same size, all margins shrink evenly. if there's no space margins will expand but elements will be centered.\", \"352765 : @CoR - the picture makes it a lot clearer what you're trying to achieve. I have a feeling you might find it tricky to do. I'd be very interested to see a solution.\", \"352260 : @Spudley: I am sorry I didn't made that picture earlier. Your solution is very near of what I want, yet very far in some way. I hope there is a relatively simple solution. If not, I'll use your answer with custom padding. Thank you for all your help :)\", \"352765 : @CoR - I've added an edit to my answer with a few final thoughts.\"]] Image path: https://i.stack.imgur.com/yDPQa.png answer:##### I recently read about a very clever technique to do exactly what you're asking.\n",
            "\n",
            "In short, you just need to use `text-align:justify;` on the container element to achieve this, in conjunction with an extra invisible block at the end.\n",
            "\n",
            "This works because `inline-block` elements are seen as being part of the text content, each being effectively a single word.\n",
            "\n",
            "Using `justify` will spread out the words in your text so that they fill the entire width of the element with extra space between the words. For `inline-block` elements, this means that they are spaced out with even spaces between them.\n",
            "\n",
            "I mentioned an extra invisible block at the end. This is required because normal `text-align:justify` won't justify the last line of text. For normal text, that's exactly what you want, but for aligning `inline-block` boxes, you want them all to be aligned.\n",
            "\n",
            "The solution is to add an extra invisible but 100% width element to the end of your list of `inline-block` elements. This will become effectively the last line of text, and thus the `justify` technique will work for the rest of your blocks.\n",
            "\n",
            "You can use the `:after` pseudo-selector to create the invisible element without needing to modify your markup.\n",
            "\n",
            "Here's an updated version of your jsFiddle to demonstrate: [http://jsfiddle.net/ULQwf/298/](http://jsfiddle.net/ULQwf/298/)\n",
            "\n",
            "And here's the original article that explains it in more detail: [http://www.barrelny.com/blog/text-align-justify-and-rwd/](http://www.barrelny.com/blog/text-align-justify-and-rwd/)\n",
            "\n",
            "\n",
            "One final update after seeing the image you've added to the question. (I don't have a better answer, but some additional thoughts that might be useful).\n",
            "\n",
            "Ideally what you need here is a `:last-line` selector. Then you could `text-align:justify` the main text and `text-align:center` the last line. That would do what you want.\n",
            "\n",
            "Sadly, `:last-line` isn't a valid selector (`:first-line` is, but not `:last-line`), so that's the end of that idea.\n",
            "\n",
            "A slightly more hopeful thought is `text-align-last`, which  exist as a feature. This could do exactly what you want:\n",
            "\n",
            "```\n",
            "text-align:justify;\n",
            "text-align-last:center;\n",
            "```\n",
            "\n",
            "\n",
            "Perfect.\n",
            "\n",
            "Except that it's non-standard and has very limited browser support.\n",
            "\n",
            "You can [read about here on MDN](https://developer.mozilla.org/en-US/docs/Web/CSS/text-align-last).\n",
            "\n",
            "I guess as a last resort it might be an option for you, if you can live with only partial browser support. It would at least get what you want for  of your users. But that's not really a sensible way to go.\n",
            "\n",
            "My gut feeling though is that this as as close as you're going to get. Tantalisingly close to what you want, but just not quite there. I hope I'm proved wrong, but I'll be surprised. Too bad though, because I it would seem like a perfectly logical thing to want to do.\n",
            "\n",
            "Datas 2##################################################################\n",
            "######################################################################\n",
            "14422323 Question:## I have added a `UIView` to a View of a View Controller. Say:\n",
            "\n",
            "```\n",
            "CGRect paneRect = {10.0f, 10.0f, 300.0f, 50.0f};\n",
            "UIView *pane = [[UIView alloc] initWithFrame:paneRect];\n",
            "pane.backgroundColor = [UIColor greenColor];\n",
            "pane.userInteractionEnabled = YES;\n",
            "pane.clipsToBounds = NO;\n",
            "[self.view addSubview:pane];\n",
            "```\n",
            "\n",
            "\n",
            "Then I've added a UIButton to `pane`:\n",
            "\n",
            "```\n",
            "CGRect testRect = {10.0f, 25.0f, pane.frame.size.width - 20.0f, 50.0f};\n",
            "UIButton *test = [UIButton buttonWithType:UIButtonTypeRoundedRect];\n",
            "[test setTitle:@\"test\" forState:UIControlStateNormal];\n",
            "[test setFrame:testRect];\n",
            "[pane addSubview:test];\n",
            "```\n",
            "\n",
            "\n",
            "Now half of the \"test\" button is within the pane and the other half is out. The upper half is interactive and responds to touches however the lower half is not.\n",
            "\n",
            "![enter image description here](https://i.stack.imgur.com/DOS7A.png)\n",
            "\n",
            "Is it possible to make the whole \"test\" button interactive and touchable instead of its half?\n",
            " Dialogs:#### [['1400768 : You can add `test` to the `self.view` anyway, unless you have some other requirement...', \"324362 : @nhahtdh In fact this is an indirect question to a bigger one. I'm developing an **Auto-Complete Text Field** and as the suggestion table-view is drawn under the text-field, I'm trying to make it a subview of the text field. everything works fine except UITableView which not receives touch events and I do not know how to deal with that.\", '1400768 : @anonim.developer: There is no need for the table view to be subview of text field. You only need to make it appears under the text field.']] Image path: https://i.stack.imgur.com/DOS7A.png answer:##### If you really want to make these view stick with each other then you can do some thing like this\n",
            "\n",
            "```\n",
            "CGRect mainViewRect = {10.0f, 10.0f, 300.0f, 75.0f};\n",
            "UIView *mainView = [[UIView alloc] initWithFrame:mainViewRect];\n",
            "mainView.backgroundColor = [UIColor clearColor];\n",
            "mainView.userInteractionEnabled = YES;\n",
            "[self.view addSubview:mainView];\n",
            "\n",
            "CGRect paneRect = {0.0f, 0.0f, 300.0f, 50.0f};\n",
            "UIView *pane = [[UIView alloc] initWithFrame:paneRect];\n",
            "pane.backgroundColor = [UIColor greenColor];\n",
            "pane.userInteractionEnabled = YES;\n",
            "pane.clipsToBounds = NO;\n",
            "[mainView addSubview:pane];\n",
            "\n",
            "\n",
            "\n",
            "CGRect testRect = {10.0f, 25.0f, pane.frame.size.width - 20.0f, 50.0f};\n",
            "UIButton *test = [UIButton buttonWithType:UIButtonTypeRoundedRect];\n",
            "[test setTitle:@\"test\" forState:UIControlStateNormal];\n",
            "[test setFrame:testRect];\n",
            "[mainView addSubview:test];\n",
            "```\n",
            "\n",
            "\n",
            "Datas 3##################################################################\n",
            "######################################################################\n",
            "10650059 Question:## We have a file repository containing code files `*.sql; *.xep; *.dll; *.aspx; *.gif` that you are going to submit to Production pretty soon. In this repository we have the main folders which contain the most recent code files to go to PROD, but we also have all the Change Folders that where submitted to PRE-PROD with code. You can see the main repository structure in the image below:\n",
            "\n",
            "![enter image description here](https://i.stack.imgur.com/Iode8.jpg)\n",
            "\n",
            "The protocol is that whenever we submit something to PRE-PROD, we e create a Change Folder, place it in the main repository, and also update the main folders, but sometimes we forget to do the second part.\n",
            "What i was trying to do in a automated way is: if there is a file with the same name in the main folder and the change folder they need the have the same modified date day (at least), this specific crossing i could easily do in Excel or even SQL.\n",
            "So, finally :P , what i needed help in, is getting into a csv fileformat (';' separating values, and '\\n' separating rows), all the `*.sql; *.xep; *.dll; *.aspx; *.gif` from the main repository directory and sub-directories.\n",
            "\n",
            "So far I i have tested with this dos commands:\n",
            "\n",
            "```\n",
            "dir *.sql *.xep *.aspx *.dll *.gif /s /a:-D>listWithDate.txt\n",
            "```\n",
            "\n",
            "\n",
            "this one gets me a list, ie: listWithDate.txt, that a i have formated in this fashion:\n",
            "\n",
            "```\n",
            "2012/03/19[2sapces]14:27[Nspaces]4.006[1space][filename]\n",
            "2012/03/19[2sapces]14:27[Nspaces]10.006[1space][filename]\n",
            "```\n",
            "\n",
            "\n",
            "So needed help in on of the two:\n",
            "\n",
            "\n",
            "\n",
            "or\n",
            "\n",
            "\n",
            "\n",
            "Many thanks in advance ;)\n",
            " Dialogs:#### [['343955 : Basically, you want to generate a CSV using the list, if I am right?', '799586 : @adarshr I was editing at the same as you, and mine had more editing so I think it took precedence over yours.', '343955 : @BaliC Not a problem, I have fixed now. By the way, it was David who was editing then. Not me.', '1384237 : tks for the editing fellows :)\\n@adarshr yes basically that, just need 2 cols, Date Modified, and File Name.']] Image path: https://i.stack.imgur.com/Iode8.jpg answer:##### The following one-liner will output file timestamps (as single units), file sizes (in bytes), and file names as a CSV row set:\n",
            "\n",
            "```\n",
            ">csvfile.txt (FOR %R IN (*.sql *.xep *.aspx *.dll *.gif) DO @ECHO \"%~tR\";%~zR;\"%~nxR\")\n",
            "```\n",
            "\n",
            "\n",
            "Note: this is supposed to be run directly from the command prompt. If you wish to put this in a batch script, double every `%` character.\n",
            "\n",
            "Datas 4##################################################################\n",
            "######################################################################\n",
            "24825347 Question:## Below is a view of the package explorer of my project. I get this error on the project root and i am not able to find which is the problem. The project runs correctly without any error in the stack trace. ![enter image description here](https://i.stack.imgur.com/p720I.jpg)\n",
            " Dialogs:#### [[\"984375 : The stack trace is empty. I only get the error sign on my project's root.\", '1113392 : @laura if you cant find the Problems tab go to `Window > show view > Problems` or just hit `Alt+Shift+Q` then hit `X`', '984375 : @A4L i followed your suggestion: there are 2 errors : `Cannot change version of project Dynamic Web Module to 2.5`', '1113392 : @laura make sure that the dependencies in your `pom.xml` and the setting in `Project > Properties > Project Facets` and doctype or schema definition in the `web.xml` are consistent', '984375 : @A4L the version of the Dynamic web module is 2.3 in Project Facets and i am not able to change it to 2.5 like the version in the pom.xml']] Image path: https://i.stack.imgur.com/p720I.jpg answer:##### \n",
            "1. Make sure that your pom.xml contains this dependency (see the version) <dependency>\n",
            "    <groupId>javax.servlet</groupId>\n",
            "    <artifactId>servlet-api</artifactId>\n",
            "    <version>2.5</version>\n",
            "</dependency>\n",
            "2. Make sure that your web.xml stats with the following schema declaration <?xml version=\"1.0\" encoding=\"UTF-8\"?>\n",
            "<web-app xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\"\n",
            "    xmlns=\"http://java.sun.com/xml/ns/javaee\"\n",
            "    xsi:schemaLocation=\"http://java.sun.com/xml/ns/javaee \n",
            "        http://java.sun.com/xml/ns/javaee/web-app_2_5.xsd\"\n",
            "    id=\"WebApp_ID\" version=\"2.5\">\n",
            "3. Make sure that your Facet config for Dynamic Web Module is set to 2.5.\n",
            "\n",
            "\n",
            "\n",
            "Hit Alt+F5 to refresh the maven Project.\n",
            "\n",
            "If you get stuck on step 3, try to delete the project from eclipse (without deleting the content!) and reimport it as maven project.\n",
            "\n",
            "Although if your just starting up, I would recommend using a newer version of the servlet spec (at least 3.0).\n",
            "\n",
            "Datas 5##################################################################\n",
            "######################################################################\n",
            "18777525 Question:## Ok, if you go here:  [http://devs.dream-portal.net/smf205/index.php?action=forum](http://devs.dream-portal.net/smf205/index.php?action=forum)\n",
            "\n",
            "You will notice a table element that contains (under the menu) a forum (board index) on the right and 2 blocks of content on the left.  All of this is within a table element with the class `dp_main` on the table element.  On the right is SMF content and here's where it gets tricky.  Ok, this `td` element has an id of `smf_col`  I need to take out ALL HTML from within the `#smf_col` td element and place it just before (or in the same spot in the DOM) the table element is.  Than I need to remove the table element `.dp_main` altogether from the DOM (and all of it's contents), than place all contents from within the `body` tag into the EMPTY `#smf_col` td element of the table, and than put that table into the `body` tag.\n",
            "\n",
            "I can only do this in the body tag, so that's why the table needs to be removed from the DOM and placed back into it once the entire `body` contents gets placed into the td element with id = `smf_col`.\n",
            "\n",
            "Using the following jQuery (a lot of manipulating here, because I can only do this in the `body` tag):\n",
            "\n",
            "```\n",
            "<script type=\"text/javascript\" src=\"//ajax.googleapis.com/ajax/libs/jquery/1/jquery.min.js\"></script>\n",
            "<script type=\"text/javascript\">\n",
            "jQuery(document).ready(function($) {\n",
            "\n",
            "    var $smf_content = $(\"#smf_col\").contents();\n",
            "    $(\"#smf_col\").empty();\n",
            "\n",
            "    $($smf_content).insertBefore($(\".dp_main\"));\n",
            "\n",
            "    var $dptable = $(\".dp_main\").contents();\n",
            "    $(\".dp_main\").remove();\n",
            "\n",
            "    var $body = $(\"body\").contents();\n",
            "    $(\"body\").empty();\n",
            "    $(\"body\").html($dptable);\n",
            "    $(\"#smf_col\").html($body);\n",
            "});\n",
            "\n",
            "</script>\n",
            "```\n",
            "\n",
            "\n",
            "The page is here:  [http://devs.dream-portal.net/smf205/index.php?action=forum](http://devs.dream-portal.net/smf205/index.php?action=forum)\n",
            "\n",
            "I will disable my code for now, since it doesn't work anyways, and leave it in it's original state so you can see exactly what I am talking about, before manipulating anything, this is what I have to work with.  Basically, this this is done properly, the 2 blocks on the left should be ALL the way to the left and the rest of the page should be on the right.\n",
            "\n",
            "Final Result should look something like the image below:\n",
            "![enter image description here](https://i.stack.imgur.com/cwvjc.jpg)\n",
            " Dialogs:#### [['114251 : did you try the update', '1443501 : @ArunPJohny - What update?', '114251 : @SolomonClosson in the chat....', \"1443501 : @ArunPJohny I'm in the chat room right now, don't see any update...  Can you meet me in there?\"]] Image path: https://i.stack.imgur.com/cwvjc.jpg answer:##### Try\n",
            "\n",
            "```\n",
            "var $dpmain = jQuery('.dp_main');\n",
            "var $body = jQuery('body');\n",
            "var $col = jQuery('#smf_col');\n",
            "\n",
            "var $ct = jQuery('<div />').insertBefore($dpmain);\n",
            "$body.append($dpmain);\n",
            "\n",
            "$col.contents().appendTo($ct);\n",
            "$body.contents().not($dpmain).appendTo($col)\n",
            "```\n",
            "\n",
            "\n",
            "Datas 6##################################################################\n",
            "######################################################################\n",
            "45469147 Question:## I was reproducing some all scripts (coded over a year ago) and found out that I am no longer getting the same plots. I am using the same dataset and the same code; the only difference is the version of my R installation and ggplot2---so I am assuming that is the problem here.\n",
            "\n",
            "Let me show you the problem with a couple of silly plots. When producing stacked barplots with percentage labels I would do something like:\n",
            "\n",
            "```\n",
            "ess2 <- ddply(ess, .(essround2), function(.){\n",
            "res <- cumsum(prop.table(table(factor(.$contplt2))))\n",
            "  res2 <- prop.table(table(factor(.$contplt2)))\n",
            "  data.frame(lab=names(res), y=c(res), res2=res2, pos=cumsum(res2)-0.5*res2)\n",
            "})\n",
            "\n",
            "ggplot(ess[ess$contplt2!=\"NA\",], aes(x=essround2))+\n",
            "  geom_bar(aes(fill=contplt2), position=\"fill\")+\n",
            "  geom_text(data=ess2[ess2$lab!=\"NA\",],\n",
            "            aes(label=round(res2.Freq, 2), x=essround2, y=pos.Freq))+\n",
            "  labs(x=\"ESS Round\", y=\"Percent\")+\n",
            "  scale_x_discrete(breaks=c(\"R1\", \"R2\", \"R3\", \"R4\", \"R5\", \"R6\"),\n",
            "                   labels=c(\"2002\", \"2004\", \"2006\", \"2008\", \"2010\", \"2012\"))+\n",
            "  ggtitle(\"Contacted politicians\")+\n",
            "  scale_fill_manual(name=\"Contacted politician\", values=c(\"#31a354\", \"#a1d99b\"))\n",
            "```\n",
            "\n",
            "\n",
            "The result would be something like: \n",
            "\n",
            "![Stacked barplot#1](https://i.stack.imgur.com/QXojR.png)\n",
            "\n",
            "As today, if I try the exact same code with the exact same dataset, I get the following plot: \n",
            "\n",
            "![Stacked barplot#2](https://i.stack.imgur.com/MhSsQ.png)\n",
            "\n",
            "As you can see the labels are not positioned properly on the bars, and the colors get inverted making the reading of the plot awkward (as if stacked barplots were not awkward enough already).\n",
            "\n",
            "Sorry for not giving you reproducible code, but I believe my problem is just me not updating my code as ggplot2 developed (or maybe is plyr the problem?) If you can spot something \"old\" in my code that might be producing the second, wonky plot I would be very grateful and happy to investigate from there myself.\n",
            "\n",
            "Thanks!!!\n",
            "\n",
            "EDIT: thanks to a suggestion in the comments, the percentages in the plots are different because I used different countries (but the same code and the same dataset). I produced the exact-exact same plot with a different version of R and ggplot2 and you can see that the problem persists: ![Stacked barplot#3](https://i.stack.imgur.com/PWw7S.png)\n",
            " Dialogs:#### [['6678410 : Thanks for your response. The second plot was produced having converted the variable \"contplt2\" to a factor with those two levels. I might not be understanding fully your suggestion: I\\'ll re-factor the variable again, but as I mentioned above it already had those two levels plus a third one labelled to \"NA\" (which I omit in the code)', '1166684 : @YSC You have also different percentages in the plot! Why ? Is your dataset changed ?', \"6678410 : @MarcoSandri sorry, did not explain that in OP (I'll edit it). Percentages are different because the second plot I produced today and I uploaded uses a different country, but the code I'm reproducing from my old script is exactly the same (the dataset is the same too).\"]] Image path: https://i.stack.imgur.com/QXojR.png answer:##### Try switching twice the labels of `contplt2`, before and after generating `ess2`.\n",
            "Hope it can help you.\n",
            "\n",
            "```\n",
            "# Here I try to reproduce your dataset\n",
            "ess <- data.frame(\n",
            "essround2 = c(\n",
            "c(rep(2002,76),rep(2002,100-76)),\n",
            "c(rep(2004,78),rep(2004,100-78)),\n",
            "c(rep(2006,81),rep(2006,100-81)),\n",
            "c(rep(2008,79),rep(2008,100-79)),\n",
            "c(rep(2010,79),rep(2010,100-79)),\n",
            "c(rep(2012,82),rep(2012,100-82))\n",
            "),\n",
            "contplt2 = c(\n",
            "c(rep(\"No\",76),rep(\"Yes\",100-76)),\n",
            "c(rep(\"No\",78),rep(\"Yes\",100-78)),\n",
            "c(rep(\"No\",81),rep(\"Yes\",100-81)),\n",
            "c(rep(\"No\",79),rep(\"Yes\",100-79)),\n",
            "c(rep(\"No\",79),rep(\"Yes\",100-79)),\n",
            "c(rep(\"No\",82),rep(\"Yes\",100-82))\n",
            ")\n",
            ")\n",
            "\n",
            "# First switch of contplt2 levels\n",
            "ess$contplt2 <- factor(ess$contplt2, levels=levels(ess$contplt2)[c(2,1)])\n",
            "\n",
            "library(plyr)\n",
            "library(ggplot2)\n",
            "ess2 <- ddply(ess, .(essround2), function(.){\n",
            "res <- cumsum(prop.table(table(factor(.$contplt2))))\n",
            "  res2 <- prop.table(table(factor(.$contplt2)))\n",
            "  data.frame(lab=names(res), y=c(res), res2=res2, pos=cumsum(res2)-0.5*res2)\n",
            "})\n",
            "\n",
            "# Second switch of contplt2 levels\n",
            "ess$contplt2 <- factor(ess$contplt2, levels=levels(ess$contplt2)[c(2,1)])\n",
            "\n",
            "\n",
            "ggplot(ess[ess$contplt2!=\"NA\",], aes(x=essround2))+\n",
            "  geom_bar(aes(fill=contplt2), position=\"fill\")+\n",
            "  geom_text(data=ess2[ess2$lab!=\"NA\",],\n",
            "            aes(label=round(res2.Freq, 2), x=essround2, y=pos.Freq))+\n",
            "  labs(x=\"ESS Round\", y=\"Percent\")+\n",
            "  scale_x_discrete(breaks=c(\"R1\", \"R2\", \"R3\", \"R4\", \"R5\", \"R6\"),\n",
            "                   labels=c(\"2002\", \"2004\", \"2006\", \"2008\", \"2010\", \"2012\"))+\n",
            "  ggtitle(\"Contacted politicians\")+\n",
            "  scale_fill_manual(name=\"Contacted politician\", values=c(\"#a1d99b\", \"#31a354\"))\n",
            "```\n",
            "\n",
            "\n",
            "[](https://i.stack.imgur.com/Rmatu.png)\n",
            "\n",
            "Datas 7##################################################################\n",
            "######################################################################\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(i)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TVo-LN5kipYo",
        "outputId": "99adb600-e242-4fd2-8a9f-634eb368be71"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "8\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "i = 0\n",
        "for output in outputs:\n",
        "    logits = output.logits\n",
        "\n",
        "    # Convert logits to probabilities using softmax\n",
        "    probabilities = torch.softmax(logits, dim=-1)\n",
        "\n",
        "    # Get the index of the token with the highest probability (greedy decoding)\n",
        "    generated_token_index = torch.argmax(probabilities, dim=-1)\n",
        "\n",
        "    # Decode the generated token index using the tokenizer\n",
        "    generated_answer = tokenizer.decode(generated_token_index[0], skip_special_tokens=True)\n",
        "    print(f\"answer {i}###########################################################\")\n",
        "    print(\"Generated answer:\", generated_answer)\n",
        "    print(\"######################################################################\")\n",
        "    i+=1"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vxHdDcj9ikLS",
        "outputId": "9b5b376e-c6ff-4000-ff2a-c15b4a0d8f58"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "answer 0###########################################################\n",
            "Generated answer: 10 </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ subject subject subject subject subject subject subject subject subject subject subject subject subject subject subject subject subject subject subject subject subject subject subject subject subject subject subject subject subject  subject           </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </                                                                                                                                            </eeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeee </ee </ee </ </ </ </ee </eee </ </e </eeeee </e </ </e </ </eeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeee\n",
            "######################################################################\n",
            "answer 1###########################################################\n",
            "Generated answer: 10 </ </ </ </ </ </ </ </ </ user user user      </ </   </  </ </    </  </ system system exception exception exception </ </ </ </ </ </   </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </e </ </ </e </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ true true true true true true true true true true true true true true true </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </\n",
            "######################################################################\n",
            "answer 2###########################################################\n",
            "Generated answer: 10 </ </ </ </ </ </ </ </ </ user user user      </ </ </        </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </e </ </e </e </e </eeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeee </eee true </ </ </e true true true true true true </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </\n",
            "######################################################################\n",
            "answer 3###########################################################\n",
            "Generated answer: 10 </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </   </ </ </ </ </ </ </22222222222222222222     m : : : </ </eeeeeeeeeeeeeeeeeeeeeeee </ </e </ </ </ </ </ </ </ </ </ </ </ </e </e </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </eeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeee    eeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeee </ </e </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </eeee </eeeeeeeeeeeeeeeeeee </ </e </\n",
            "######################################################################\n",
            "answer 4###########################################################\n",
            "Generated answer: 10 </ </ </ </ </ </ </ </ </ user user user      </ </   </  </ </    </  </ system system exception exception exception </ </ </ </ </ </:  </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </e </e </e </eeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeee </ee </e </ </ee </ </ </e </e </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ true </ </  </ true </ </ true true true true true </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </e </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </e </ </ </ </e </e </\n",
            "######################################################################\n",
            "answer 5###########################################################\n",
            "Generated answer: 10 </ </ </ </ </ </ </ </ </ user user user      </ </   </  </ </    </  </ system </ </ </ </ </ </ </ </ </   </ </ </ </ </. </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </e </ee </eeee </eeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeee </e </ </ true </ </  </    true  </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </eeee </eeeeeeeee </ </e </ee </ </ </ </ </ </ </ </e </ </ </ </ </e </ </ </ </e </ </ee </ </eeeee </\n",
            "######################################################################\n",
            "answer 6###########################################################\n",
            "Generated answer: 10 </ </ </ </ </ </ </ </ </ user user user  </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </        </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </222222222222222222222 : : </ </ </ </ </ </ </ </eeee </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </e </ </ee </eeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeee </eeeeeee </e true </ </e </eeeeeeee </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ee </eeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeee </e </\n",
            "######################################################################\n",
            "answer 7###########################################################\n",
            "Generated answer: 10 </ </ </ </ </   sub sub subject subject subject subject subject subject subject subject subject subject subject subject subject subject subject subject </ </ </ </ </ </ </ </ </ </ </'s's2222222222222222222 space2222 etc etc etceeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeee theme them </ them them them them themeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeee </e </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </ </aa </ </ </a </\n",
            "######################################################################\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "BN6TbCuLodqE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oa4Fo66_g--i"
      },
      "outputs": [],
      "source": [
        "results = {\n",
        "    'train_loss': [],\n",
        "    'val_loss': [],\n",
        "    'ndcg': [],\n",
        "    'mrr': [],\n",
        "    'model_state_dict': []  # to store model parameters\n",
        "}\n",
        "num_epochs_to_keep = 2"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "rOHF3XVPSsC2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# import warnings\n",
        "# warnings.filterwarnings(\"ignore\", message=\"Input length of input_ids is .* but `max_length` is .*\")\n",
        "# from torch.cuda.amp import GradScaler, autocast\n",
        "# import numpy as np\n",
        "# from sklearn.metrics import ndcg_score\n",
        "\n",
        "# scaler = GradScaler()\n",
        "\n",
        "# # Set model to train mode\n",
        "# model.train()\n",
        "# num_epochs = 5\n",
        "# sequence_length = 1024\n",
        "# best_val_loss = float('inf')\n",
        "# patience = 4\n",
        "# counter = 0\n",
        "# values_loss = []\n",
        "# values_loss_train = []\n",
        "# values_NDCG = []\n",
        "\n",
        "# for epoch in range(num_epochs):\n",
        "#     running_train_loss = 0.0\n",
        "#     running_val_loss = 0.0\n",
        "\n",
        "#     # Training loop\n",
        "#     for Qdialog_tokens, Adialog_tokens, Question_tokens, image, answer in data_loader:\n",
        "#         optimizer.zero_grad()\n",
        "#         image = image_processor(images=image, return_tensors=\"pt\").to(device)\n",
        "#         outputs ,_ = model(image, Question_tokens, Adialog_tokens, Question_tokens, answer)\n",
        "#         # print(outputs.hidden_states[-2].shape)\n",
        "#         loss = outputs.loss\n",
        "#         loss.requires_grad = True\n",
        "#         loss.backward()\n",
        "#         nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
        "#         optimizer.step()\n",
        "#         running_train_loss += loss.item()\n",
        "\n",
        "#     # Compute average training loss\n",
        "#     train_loss = running_train_loss / len(data_loader)\n",
        "#     print(train_loss)\n",
        "\n",
        "#     # Save model state\n",
        "#     torch.save(model.state_dict(), path)\n",
        "\n",
        "#     # Validation loop\n",
        "#     # model.eval()\n",
        "#     # all_ndcg_scores = []\n",
        "#     # with torch.no_grad():\n",
        "#     #     for val_Qdialog_tokens, val_Adialog_tokens, val_Question_tokens, val_image, val_answer in val_loader:\n",
        "#     #         val_image = image_processor(images=val_image, return_tensors=\"pt\").to(device)\n",
        "\n",
        "#     #         val_outputs,predicted_scores  = model(val_image, val_Question_tokens, val_Adialog_tokens, val_Question_tokens, val_answer)\n",
        "\n",
        "#     #         val_loss = val_outputs.loss\n",
        "#     #         running_val_loss += val_loss.item()\n",
        "\n",
        "#             # Compute NDCG batch-wise\n",
        "#             # print(predicted_scores.shape)\n",
        "#             # flattened_relevance_scores = val_answer.cpu().numpy().flatten()\n",
        "#             # flattened_predicted_scores = predicted_scores.cpu().numpy().flatten()\n",
        "#             # reshaped_relevance_scores = np.reshape(flattened_relevance_scores, (batch_size, sequence_length))\n",
        "#             # reshaped_predicted_scores = np.reshape(flattened_predicted_scores, (batch_size, sequence_length))\n",
        "#             # ndcg = ndcg_score(reshaped_relevance_scores, reshaped_predicted_scores)\n",
        "#             # all_ndcg_scores.append(ndcg)\n",
        "\n",
        "#     # Compute average validation loss\n",
        "#     # val_loss = running_val_loss / len(val_loader)\n",
        "\n",
        "#     # Compute average NDCG score\n",
        "#     # avg_ndcg = np.mean(all_ndcg_scores)\n",
        "\n",
        "#     # Print and store results\n",
        "#     # values_loss.append(val_loss)\n",
        "#     # values_loss_train.append(train_loss)\n",
        "#     # # values_NDCG.append(avg_ndcg)\n",
        "#     #print(f\"Epoch {epoch+1}, Train Loss: {train_loss},  NDCG: avg_ndcg\")\n",
        "\n",
        "#     # # Check for early stopping\n",
        "#     # if val_loss < best_val_loss:\n",
        "#     #     best_val_loss = val_loss\n",
        "#     #     counter = 0\n",
        "#     # else:\n",
        "#     #     counter += 1\n",
        "#     #     if counter >= patience:\n",
        "#     #         print(\"Validation loss did not improve for\", patience, \"epochs. Early stopping...\")\n",
        "#     #         break\n",
        "\n",
        "#     # model.train()\n",
        "\n",
        "# #print(\"Training finished.\")\n"
      ],
      "metadata": {
        "id": "PnBBDP4ExL_8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# image_data , question_dialog_data , question_data  = []\n",
        "# with torch.no_grad():\n",
        "#     model.eval()  # Set the model to evaluation mode\n",
        "#     output = model(images=image_data, questiondialogs=question_dialog_data, questions=question_data)\n",
        "\n",
        "# # Process the output\n",
        "# # For example, if your output is a tensor of predicted answers, you can convert it to text\n",
        "# predicted_answers = output.argmax(dim=2)  # Assuming output is logits, take the argmax along the vocabulary dimension\n",
        "\n",
        "# # Optionally, convert the predicted answers from token IDs to actual text using your tokenizer\n",
        "# predicted_answer_texts = tokenizer.decode(predicted_answers)\n",
        "\n",
        "# # Print or use the predicted answers as needed\n",
        "# print(\"Predicted answers:\", predicted_answers)"
      ],
      "metadata": {
        "id": "x9PgX3Z0tHKC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "# import torch\n",
        "\n",
        "\n",
        "# tokenizer = AutoTokenizer.from_pretrained(\"microsoft/DialoGPT-medium\")\n",
        "# model = AutoModelForCausalLM.from_pretrained(\"microsoft/DialoGPT-medium\")\n",
        "\n",
        "# # Let's chat for 5 lines\n",
        "# for step in range(5):\n",
        "#     # encode the new user input, add the eos_token and return a tensor in Pytorch\n",
        "#     new_user_input_ids = tokenizer.encode(input(\">> User:\") + tokenizer.eos_token, return_tensors='pt')\n",
        "\n",
        "#     # append the new user input tokens to the chat history\n",
        "#     bot_input_ids = torch.cat([chat_history_ids, new_user_input_ids], dim=-1) if step > 0 else new_user_input_ids\n",
        "\n",
        "#     # generated a response while limiting the total chat history to 1000 tokens,\n",
        "#     chat_history_ids = model.generate(bot_input_ids, max_length=1000, pad_token_id=tokenizer.eos_token_id)\n",
        "\n",
        "#     # pretty print last ouput tokens from bot\n",
        "#     print(\"DialoGPT: {}\".format(tokenizer.decode(chat_history_ids[:, bot_input_ids.shape[-1]:][0], skip_special_tokens=True)))\n"
      ],
      "metadata": {
        "id": "2EsOwyrnuEfH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# path = os.path.join(path,'/content/sample_data')\n",
        "# torch.save(model,\"/content/drive/MyDrive/model2/FILE_NAME')"
      ],
      "metadata": {
        "id": "KcZ5tZMY-0M2"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "cell_execution_strategy": "setup",
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}